{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aefc6804-7232-43f4-9d6c-918f8eda6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "391a1eb7-e940-4968-b7bd-1a42ddec0d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package keras.api._v2.keras.datasets in keras.api._v2.keras:\n",
      "\n",
      "NAME\n",
      "    keras.api._v2.keras.datasets - AUTOGENERATED. DO NOT EDIT.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    boston_housing (package)\n",
      "    cifar10 (package)\n",
      "    cifar100 (package)\n",
      "    fashion_mnist (package)\n",
      "    imdb (package)\n",
      "    mnist (package)\n",
      "    reuters (package)\n",
      "\n",
      "FILE\n",
      "    c:\\users\\tarun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages\\keras\\api\\_v2\\keras\\datasets\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45da8be7-5691-4c02-b791-2f204639fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset.\n",
    "dataset = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03544fd9-7a35-4c0f-aee2-1a3abc89d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02dd93d5-72a8-44da-ba1f-9dc76d5d483e",
   "metadata": {},
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08dbdfd9-a6c6-41d7-b821-871d6a6d0070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_data in module keras.src.datasets.cifar10:\n",
      "\n",
      "load_data()\n",
      "    Loads the CIFAR10 dataset.\n",
      "    \n",
      "    This is a dataset of 50,000 32x32 color training images and 10,000 test\n",
      "    images, labeled over 10 categories. See more info at the\n",
      "    [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
      "    \n",
      "    The classes are:\n",
      "    \n",
      "    | Label | Description |\n",
      "    |:-----:|-------------|\n",
      "    |   0   | airplane    |\n",
      "    |   1   | automobile  |\n",
      "    |   2   | bird        |\n",
      "    |   3   | cat         |\n",
      "    |   4   | deer        |\n",
      "    |   5   | dog         |\n",
      "    |   6   | frog        |\n",
      "    |   7   | horse       |\n",
      "    |   8   | ship        |\n",
      "    |   9   | truck       |\n",
      "    \n",
      "    Returns:\n",
      "      Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
      "    \n",
      "    **x_train**: uint8 NumPy array of grayscale image data with shapes\n",
      "      `(50000, 32, 32, 3)`, containing the training data. Pixel values range\n",
      "      from 0 to 255.\n",
      "    \n",
      "    **y_train**: uint8 NumPy array of labels (integers in range 0-9)\n",
      "      with shape `(50000, 1)` for the training data.\n",
      "    \n",
      "    **x_test**: uint8 NumPy array of grayscale image data with shapes\n",
      "      `(10000, 32, 32, 3)`, containing the test data. Pixel values range\n",
      "      from 0 to 255.\n",
      "    \n",
      "    **y_test**: uint8 NumPy array of labels (integers in range 0-9)\n",
      "      with shape `(10000, 1)` for the test data.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
      "    assert x_train.shape == (50000, 32, 32, 3)\n",
      "    assert x_test.shape == (10000, 32, 32, 3)\n",
      "    assert y_train.shape == (50000, 1)\n",
      "    assert y_test.shape == (10000, 1)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.datasets.cifar10.load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a70fd22f-0e43-4dbc-82c8-64f62afbed1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ImageDataGenerator in module keras.src.preprocessing.image:\n",
      "\n",
      "class ImageDataGenerator(builtins.object)\n",
      " |  ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, interpolation_order=1, dtype=None)\n",
      " |  \n",
      " |  Generate batches of tensor image data with real-time data augmentation.\n",
      " |  \n",
      " |  Deprecated: `tf.keras.preprocessing.image.ImageDataGenerator` is not\n",
      " |  recommended for new code. Prefer loading images with\n",
      " |  `tf.keras.utils.image_dataset_from_directory` and transforming the output\n",
      " |  `tf.data.Dataset` with preprocessing layers. For more information, see the\n",
      " |  tutorials for [loading images](\n",
      " |  https://www.tensorflow.org/tutorials/load_data/images) and\n",
      " |  [augmenting images](\n",
      " |  https://www.tensorflow.org/tutorials/images/data_augmentation), as well as\n",
      " |  the [preprocessing layer guide](\n",
      " |  https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
      " |  \n",
      " |   The data will be looped over (in batches).\n",
      " |  \n",
      " |  Args:\n",
      " |      featurewise_center: Boolean. Set input mean to 0 over the dataset,\n",
      " |        feature-wise.\n",
      " |      samplewise_center: Boolean. Set each sample mean to 0.\n",
      " |      featurewise_std_normalization: Boolean. Divide inputs by std of the\n",
      " |        dataset, feature-wise.\n",
      " |      samplewise_std_normalization: Boolean. Divide each input by its std.\n",
      " |      zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
      " |      zca_whitening: Boolean. Apply ZCA whitening.\n",
      " |      rotation_range: Int. Degree range for random rotations.\n",
      " |      width_shift_range: Float, 1-D array-like or int\n",
      " |          - float: fraction of total width, if < 1, or pixels if >= 1.\n",
      " |          - 1-D array-like: random elements from the array.\n",
      " |          - int: integer number of pixels from interval `(-width_shift_range,\n",
      " |            +width_shift_range)` - With `width_shift_range=2` possible values\n",
      " |            are integers `[-1, 0, +1]`, same as with `width_shift_range=[-1,\n",
      " |            0, +1]`, while with `width_shift_range=1.0` possible values are\n",
      " |            floats in the interval [-1.0, +1.0).\n",
      " |      height_shift_range: Float, 1-D array-like or int\n",
      " |          - float: fraction of total height, if < 1, or pixels if >= 1.\n",
      " |          - 1-D array-like: random elements from the array.\n",
      " |          - int: integer number of pixels from interval `(-height_shift_range,\n",
      " |            +height_shift_range)` - With `height_shift_range=2` possible\n",
      " |            values are integers `[-1, 0, +1]`, same as with\n",
      " |            `height_shift_range=[-1, 0, +1]`, while with\n",
      " |            `height_shift_range=1.0` possible values are floats in the\n",
      " |            interval [-1.0, +1.0).\n",
      " |      brightness_range: Tuple or list of two floats. Range for picking a\n",
      " |        brightness shift value from.\n",
      " |      shear_range: Float. Shear Intensity (Shear angle in counter-clockwise\n",
      " |        direction in degrees)\n",
      " |      zoom_range: Float or [lower, upper]. Range for random zoom. If a float,\n",
      " |        `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
      " |      channel_shift_range: Float. Range for random channel shifts.\n",
      " |      fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default\n",
      " |        is 'nearest'. Points outside the boundaries of the input are filled\n",
      " |        according to the given mode:\n",
      " |          - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
      " |          - 'nearest':  aaaaaaaa|abcd|dddddddd\n",
      " |          - 'reflect':  abcddcba|abcd|dcbaabcd\n",
      " |          - 'wrap':  abcdabcd|abcd|abcdabcd\n",
      " |      cval: Float or Int. Value used for points outside the boundaries when\n",
      " |        `fill_mode = \"constant\"`.\n",
      " |      horizontal_flip: Boolean. Randomly flip inputs horizontally.\n",
      " |      vertical_flip: Boolean. Randomly flip inputs vertically.\n",
      " |      rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
      " |        is applied, otherwise we multiply the data by the value provided\n",
      " |        (after applying all other transformations).\n",
      " |      preprocessing_function: function that will be applied on each input. The\n",
      " |        function will run after the image is resized and augmented.\n",
      " |          The function should take one argument: one image (Numpy tensor with\n",
      " |            rank 3), and should output a Numpy tensor with the same shape.\n",
      " |      data_format: Image data format, either \"channels_first\" or\n",
      " |        \"channels_last\". \"channels_last\" mode means that the images should\n",
      " |        have shape `(samples, height, width, channels)`, \"channels_first\" mode\n",
      " |        means that the images should have shape `(samples, channels, height,\n",
      " |        width)`.  It defaults to the `image_data_format` value found in your\n",
      " |        Keras config file at `~/.keras/keras.json`. If you never set it, then\n",
      " |        it will be \"channels_last\".\n",
      " |      validation_split: Float. Fraction of images reserved for validation\n",
      " |        (strictly between 0 and 1).\n",
      " |      dtype: Dtype to use for the generated arrays.\n",
      " |  \n",
      " |  Raises:\n",
      " |    ValueError: If the value of the argument, `data_format` is other than\n",
      " |          `\"channels_last\"` or `\"channels_first\"`.\n",
      " |    ValueError: If the value of the argument, `validation_split` > 1\n",
      " |          or `validation_split` < 0.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  Example of using `.flow(x, y)`:\n",
      " |  \n",
      " |  ```python\n",
      " |  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
      " |  y_train = utils.to_categorical(y_train, num_classes)\n",
      " |  y_test = utils.to_categorical(y_test, num_classes)\n",
      " |  datagen = ImageDataGenerator(\n",
      " |      featurewise_center=True,\n",
      " |      featurewise_std_normalization=True,\n",
      " |      rotation_range=20,\n",
      " |      width_shift_range=0.2,\n",
      " |      height_shift_range=0.2,\n",
      " |      horizontal_flip=True,\n",
      " |      validation_split=0.2)\n",
      " |  # compute quantities required for featurewise normalization\n",
      " |  # (std, mean, and principal components if ZCA whitening is applied)\n",
      " |  datagen.fit(x_train)\n",
      " |  # fits the model on batches with real-time data augmentation:\n",
      " |  model.fit(datagen.flow(x_train, y_train, batch_size=32,\n",
      " |           subset='training'),\n",
      " |           validation_data=datagen.flow(x_train, y_train,\n",
      " |           batch_size=8, subset='validation'),\n",
      " |           steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
      " |  # here's a more \"manual\" example\n",
      " |  for e in range(epochs):\n",
      " |      print('Epoch', e)\n",
      " |      batches = 0\n",
      " |      for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
      " |          model.fit(x_batch, y_batch)\n",
      " |          batches += 1\n",
      " |          if batches >= len(x_train) / 32:\n",
      " |              # we need to break the loop by hand because\n",
      " |              # the generator loops indefinitely\n",
      " |              break\n",
      " |  ```\n",
      " |  \n",
      " |  Example of using `.flow_from_directory(directory)`:\n",
      " |  \n",
      " |  ```python\n",
      " |  train_datagen = ImageDataGenerator(\n",
      " |          rescale=1./255,\n",
      " |          shear_range=0.2,\n",
      " |          zoom_range=0.2,\n",
      " |          horizontal_flip=True)\n",
      " |  test_datagen = ImageDataGenerator(rescale=1./255)\n",
      " |  train_generator = train_datagen.flow_from_directory(\n",
      " |          'data/train',\n",
      " |          target_size=(150, 150),\n",
      " |          batch_size=32,\n",
      " |          class_mode='binary')\n",
      " |  validation_generator = test_datagen.flow_from_directory(\n",
      " |          'data/validation',\n",
      " |          target_size=(150, 150),\n",
      " |          batch_size=32,\n",
      " |          class_mode='binary')\n",
      " |  model.fit(\n",
      " |          train_generator,\n",
      " |          steps_per_epoch=2000,\n",
      " |          epochs=50,\n",
      " |          validation_data=validation_generator,\n",
      " |          validation_steps=800)\n",
      " |  ```\n",
      " |  \n",
      " |  Example of transforming images and masks together.\n",
      " |  \n",
      " |  ```python\n",
      " |  # we create two instances with the same arguments\n",
      " |  data_gen_args = dict(featurewise_center=True,\n",
      " |                       featurewise_std_normalization=True,\n",
      " |                       rotation_range=90,\n",
      " |                       width_shift_range=0.1,\n",
      " |                       height_shift_range=0.1,\n",
      " |                       zoom_range=0.2)\n",
      " |  image_datagen = ImageDataGenerator(**data_gen_args)\n",
      " |  mask_datagen = ImageDataGenerator(**data_gen_args)\n",
      " |  # Provide the same seed and keyword arguments to the fit and flow methods\n",
      " |  seed = 1\n",
      " |  image_datagen.fit(images, augment=True, seed=seed)\n",
      " |  mask_datagen.fit(masks, augment=True, seed=seed)\n",
      " |  image_generator = image_datagen.flow_from_directory(\n",
      " |      'data/images',\n",
      " |      class_mode=None,\n",
      " |      seed=seed)\n",
      " |  mask_generator = mask_datagen.flow_from_directory(\n",
      " |      'data/masks',\n",
      " |      class_mode=None,\n",
      " |      seed=seed)\n",
      " |  # combine generators into one which yields image and masks\n",
      " |  train_generator = zip(image_generator, mask_generator)\n",
      " |  model.fit(\n",
      " |      train_generator,\n",
      " |      steps_per_epoch=2000,\n",
      " |      epochs=50)\n",
      " |  ```\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, interpolation_order=1, dtype=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  apply_transform(self, x, transform_parameters)\n",
      " |      Applies a transformation to an image according to given parameters.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: 3D tensor, single image.\n",
      " |          transform_parameters: Dictionary with string - parameter pairs\n",
      " |              describing the transformation.\n",
      " |              Currently, the following parameters\n",
      " |              from the dictionary are used:\n",
      " |              - `'theta'`: Float. Rotation angle in degrees.\n",
      " |              - `'tx'`: Float. Shift in the x direction.\n",
      " |              - `'ty'`: Float. Shift in the y direction.\n",
      " |              - `'shear'`: Float. Shear angle in degrees.\n",
      " |              - `'zx'`: Float. Zoom in the x direction.\n",
      " |              - `'zy'`: Float. Zoom in the y direction.\n",
      " |              - `'flip_horizontal'`: Boolean. Horizontal flip.\n",
      " |              - `'flip_vertical'`: Boolean. Vertical flip.\n",
      " |              - `'channel_shift_intensity'`: Float. Channel shift intensity.\n",
      " |              - `'brightness'`: Float. Brightness shift intensity.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A transformed version of the input (same shape).\n",
      " |  \n",
      " |  fit(self, x, augment=False, rounds=1, seed=None)\n",
      " |      Fits the data generator to some sample data.\n",
      " |      \n",
      " |      This computes the internal data stats related to the\n",
      " |      data-dependent transformations, based on an array of sample data.\n",
      " |      \n",
      " |      Only required if `featurewise_center` or\n",
      " |      `featurewise_std_normalization` or `zca_whitening` are set to True.\n",
      " |      \n",
      " |      When `rescale` is set to a value, rescaling is applied to\n",
      " |      sample data before computing the internal data stats.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Sample data. Should have rank 4.\n",
      " |           In case of grayscale data,\n",
      " |           the channels axis should have value 1, in case\n",
      " |           of RGB data, it should have value 3, and in case\n",
      " |           of RGBA data, it should have value 4.\n",
      " |          augment: Boolean (default: False).\n",
      " |              Whether to fit on randomly augmented samples.\n",
      " |          rounds: Int (default: 1).\n",
      " |              If using data augmentation (`augment=True`),\n",
      " |              this is how many augmentation passes over the data to use.\n",
      " |          seed: Int (default: None). Random seed.\n",
      " |  \n",
      " |  flow(self, x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_prefix='', save_format='png', ignore_class_split=False, subset=None)\n",
      " |      Takes data & label arrays, generates batches of augmented data.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. Numpy array of rank 4 or a tuple. If tuple, the first\n",
      " |            element should contain the images and the second element another\n",
      " |            numpy array or a list of numpy arrays that gets passed to the\n",
      " |            output without any modifications. Can be used to feed the model\n",
      " |            miscellaneous data along with the images. In case of grayscale\n",
      " |            data, the channels axis of the image array should have value 1, in\n",
      " |            case of RGB data, it should have value 3, and in case of RGBA\n",
      " |            data, it should have value 4.\n",
      " |          y: Labels.\n",
      " |          batch_size: Int (default: 32).\n",
      " |          shuffle: Boolean (default: True).\n",
      " |          sample_weight: Sample weights.\n",
      " |          seed: Int (default: None).\n",
      " |          save_to_dir: None or str (default: None). This allows you to\n",
      " |            optionally specify a directory to which to save the augmented\n",
      " |            pictures being generated (useful for visualizing what you are\n",
      " |            doing).\n",
      " |          save_prefix: Str (default: `''`). Prefix to use for filenames of\n",
      " |            saved pictures (only relevant if `save_to_dir` is set).\n",
      " |          save_format: one of \"png\", \"jpeg\", \"bmp\", \"pdf\", \"ppm\", \"gif\",\n",
      " |            \"tif\", \"jpg\" (only relevant if `save_to_dir` is set). Default:\n",
      " |            \"png\".\n",
      " |          ignore_class_split: Boolean (default: False), ignore difference\n",
      " |            in number of classes in labels across train and validation\n",
      " |            split (useful for non-classification tasks)\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |            `validation_split` is set in `ImageDataGenerator`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An `Iterator` yielding tuples of `(x, y)`\n",
      " |              where `x` is a numpy array of image data\n",
      " |              (in the case of a single image input) or a list\n",
      " |              of numpy arrays (in the case with\n",
      " |              additional inputs) and `y` is a numpy array\n",
      " |              of corresponding labels. If 'sample_weight' is not None,\n",
      " |              the yielded tuples are of the form `(x, y, sample_weight)`.\n",
      " |              If `y` is None, only the numpy array `x` is returned.\n",
      " |      Raises:\n",
      " |        ValueError: If the Value of the argument, `subset` is other than\n",
      " |              \"training\" or \"validation\".\n",
      " |  \n",
      " |  flow_from_dataframe(self, dataframe, directory=None, x_col='filename', y_col='class', weight_col=None, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None, interpolation='nearest', validate_filenames=True, **kwargs)\n",
      " |      Takes the dataframe and the path to a directory + generates batches.\n",
      " |      \n",
      " |       The generated batches contain augmented/normalized data.\n",
      " |      \n",
      " |      **A simple tutorial can be found **[here](\n",
      " |                                  http://bit.ly/keras_flow_from_dataframe).\n",
      " |      \n",
      " |      Args:\n",
      " |          dataframe: Pandas dataframe containing the filepaths relative to\n",
      " |              `directory` (or absolute paths if `directory` is None) of the\n",
      " |              images in a string column. It should include other column/s\n",
      " |              depending on the `class_mode`:\n",
      " |              - if `class_mode` is `\"categorical\"` (default value) it must\n",
      " |                  include the `y_col` column with the class/es of each image.\n",
      " |                  Values in column can be string/list/tuple if a single class\n",
      " |                  or list/tuple if multiple classes.\n",
      " |              - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
      " |                  the given `y_col` column with class values as strings.\n",
      " |              - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should\n",
      " |                  contain the columns specified in `y_col`.\n",
      " |              - if `class_mode` is `\"input\"` or `None` no extra column is\n",
      " |                  needed.\n",
      " |          directory: string, path to the directory to read images from. If\n",
      " |            `None`, data in `x_col` column should be absolute paths.\n",
      " |          x_col: string, column in `dataframe` that contains the filenames (or\n",
      " |            absolute paths if `directory` is `None`).\n",
      " |          y_col: string or list, column/s in `dataframe` that has the target\n",
      " |            data.\n",
      " |          weight_col: string, column in `dataframe` that contains the sample\n",
      " |              weights. Default: `None`.\n",
      " |          target_size: tuple of integers `(height, width)`, default: `(256,\n",
      " |            256)`. The dimensions to which all images found will be resized.\n",
      " |          color_mode: one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
      " |            Whether the images will be converted to have 1 or 3 color\n",
      " |            channels.\n",
      " |          classes: optional list of classes (e.g. `['dogs', 'cats']`). Default\n",
      " |            is None. If not provided, the list of classes will be\n",
      " |            automatically inferred from the `y_col`, which will map to the\n",
      " |            label indices, will be alphanumeric). The dictionary containing\n",
      " |            the mapping from class names to class indices can be obtained via\n",
      " |            the attribute `class_indices`.\n",
      " |          class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
      " |              \"raw\", sparse\" or None. Default: \"categorical\".\n",
      " |              Mode for yielding the targets:\n",
      " |              - `\"binary\"`: 1D numpy array of binary labels,\n",
      " |              - `\"categorical\"`: 2D numpy array of one-hot encoded labels.\n",
      " |                Supports multi-label output.\n",
      " |              - `\"input\"`: images identical to input images (mainly used to\n",
      " |                work with autoencoders),\n",
      " |              - `\"multi_output\"`: list with the values of the different\n",
      " |                columns,\n",
      " |              - `\"raw\"`: numpy array of values in `y_col` column(s),\n",
      " |              - `\"sparse\"`: 1D numpy array of integer labels,\n",
      " |              - `None`, no targets are returned (the generator will only yield\n",
      " |                batches of image data, which is useful to use in\n",
      " |                `model.predict()`).\n",
      " |          batch_size: size of the batches of data (default: 32).\n",
      " |          shuffle: whether to shuffle the data (default: True)\n",
      " |          seed: optional random seed for shuffling and transformations.\n",
      " |          save_to_dir: None or str (default: None). This allows you to\n",
      " |            optionally specify a directory to which to save the augmented\n",
      " |            pictures being generated (useful for visualizing what you are\n",
      " |            doing).\n",
      " |          save_prefix: str. Prefix to use for filenames of saved pictures\n",
      " |            (only relevant if `save_to_dir` is set).\n",
      " |          save_format: one of \"png\", \"jpeg\", \"bmp\", \"pdf\", \"ppm\", \"gif\",\n",
      " |            \"tif\", \"jpg\" (only relevant if `save_to_dir` is set). Default:\n",
      " |            \"png\".\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |            `validation_split` is set in `ImageDataGenerator`.\n",
      " |          interpolation: Interpolation method used to resample the image if\n",
      " |            the target size is different from that of the loaded image.\n",
      " |            Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
      " |            If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
      " |            supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
      " |            `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
      " |          validate_filenames: Boolean, whether to validate image filenames in\n",
      " |            `x_col`. If `True`, invalid images will be ignored. Disabling this\n",
      " |            option can lead to speed-up in the execution of this function.\n",
      " |            Defaults to `True`.\n",
      " |          **kwargs: legacy arguments for raising deprecation warnings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `DataFrameIterator` yielding tuples of `(x, y)`\n",
      " |          where `x` is a numpy array containing a batch\n",
      " |          of images with shape `(batch_size, *target_size, channels)`\n",
      " |          and `y` is a numpy array of corresponding labels.\n",
      " |  \n",
      " |  flow_from_directory(self, directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest', keep_aspect_ratio=False)\n",
      " |      Takes the path to a directory & generates batches of augmented data.\n",
      " |      \n",
      " |      Args:\n",
      " |          directory: string, path to the target directory. It should contain\n",
      " |            one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images\n",
      " |            inside each of the subdirectories directory tree will be included\n",
      " |            in the generator. See [this script](\n",
      " |            https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)\n",
      " |            for more details.\n",
      " |          target_size: Tuple of integers `(height, width)`, defaults to `(256,\n",
      " |            256)`. The dimensions to which all images found will be resized.\n",
      " |          color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
      " |            Whether the images will be converted to have 1, 3, or 4 channels.\n",
      " |          classes: Optional list of class subdirectories (e.g. `['dogs',\n",
      " |            'cats']`). Default: None. If not provided, the list of classes\n",
      " |            will be automatically inferred from the subdirectory\n",
      " |            names/structure under `directory`, where each subdirectory will be\n",
      " |            treated as a different class (and the order of the classes, which\n",
      " |            will map to the label indices, will be alphanumeric). The\n",
      " |            dictionary containing the mapping from class names to class\n",
      " |            indices can be obtained via the attribute `class_indices`.\n",
      " |          class_mode: One of \"categorical\", \"binary\", \"sparse\",\n",
      " |              \"input\", or None. Default: \"categorical\".\n",
      " |              Determines the type of label arrays that are returned:\n",
      " |              - \"categorical\" will be 2D one-hot encoded labels,\n",
      " |              - \"binary\" will be 1D binary labels,\n",
      " |                  \"sparse\" will be 1D integer labels,\n",
      " |              - \"input\" will be images identical\n",
      " |                  to input images (mainly used to work with autoencoders).\n",
      " |              - If None, no labels are returned\n",
      " |                (the generator will only yield batches of image data,\n",
      " |                which is useful to use with `model.predict_generator()`).\n",
      " |                Please note that in case of class_mode None,\n",
      " |                the data still needs to reside in a subdirectory\n",
      " |                of `directory` for it to work correctly.\n",
      " |          batch_size: Size of the batches of data (default: 32).\n",
      " |          shuffle: Whether to shuffle the data (default: True) If set to\n",
      " |            False, sorts the data in alphanumeric order.\n",
      " |          seed: Optional random seed for shuffling and transformations.\n",
      " |          save_to_dir: None or str (default: None). This allows you to\n",
      " |            optionally specify a directory to which to save the augmented\n",
      " |            pictures being generated (useful for visualizing what you are\n",
      " |            doing).\n",
      " |          save_prefix: Str. Prefix to use for filenames of saved pictures\n",
      " |            (only relevant if `save_to_dir` is set).\n",
      " |          save_format: one of \"png\", \"jpeg\", \"bmp\", \"pdf\", \"ppm\", \"gif\",\n",
      " |            \"tif\", \"jpg\" (only relevant if `save_to_dir` is set). Default:\n",
      " |            \"png\".\n",
      " |          follow_links: Whether to follow symlinks inside\n",
      " |              class subdirectories (default: False).\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |            `validation_split` is set in `ImageDataGenerator`.\n",
      " |          interpolation: Interpolation method used to resample the image if\n",
      " |            the target size is different from that of the loaded image.\n",
      " |            Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
      " |            If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
      " |            supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
      " |            `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
      " |          keep_aspect_ratio: Boolean, whether to resize images to a target\n",
      " |            size without aspect ratio distortion. The image is cropped in\n",
      " |            the center with target aspect ratio before resizing.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `DirectoryIterator` yielding tuples of `(x, y)`\n",
      " |              where `x` is a numpy array containing a batch\n",
      " |              of images with shape `(batch_size, *target_size, channels)`\n",
      " |              and `y` is a numpy array of corresponding labels.\n",
      " |  \n",
      " |  get_random_transform(self, img_shape, seed=None)\n",
      " |      Generates random parameters for a transformation.\n",
      " |      \n",
      " |      Args:\n",
      " |          img_shape: Tuple of integers.\n",
      " |              Shape of the image that is transformed.\n",
      " |          seed: Random seed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary containing randomly chosen parameters describing the\n",
      " |          transformation.\n",
      " |  \n",
      " |  random_transform(self, x, seed=None)\n",
      " |      Applies a random transformation to an image.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: 3D tensor, single image.\n",
      " |          seed: Random seed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A randomly transformed version of the input (same shape).\n",
      " |  \n",
      " |  standardize(self, x)\n",
      " |      Applies the normalization configuration in-place to a batch of\n",
      " |      inputs.\n",
      " |      \n",
      " |      `x` is changed in-place since the function is mainly used internally\n",
      " |      to standardize images and feed them to your network. If a copy of `x`\n",
      " |      would be created instead it would have a significant performance cost.\n",
      " |      If you want to apply this method without changing the input in-place\n",
      " |      you can call the method creating a copy before:\n",
      " |      \n",
      " |      standardize(np.copy(x))\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Batch of inputs to be normalized.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The inputs, normalized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "help(ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9aa8d1f-57f3-47bf-862c-246824f71088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "idg = ImageDataGenerator(rotation_range=20, horizontal_flip=True, vertical_flip=True)\n",
    "augmented_data_iterator = idg.flow(x_train, y_train, batch_size=len(x_train), shuffle=False)\n",
    "\n",
    "x_train, y_train = augmented_data_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "593bbc73-df05-48fb-879c-f5df6c7a4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "540f612b-dce5-4ed5-97da-210543d1206b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dense in module keras.src.layers.core.dense:\n",
      "\n",
      "class Dense(keras.src.engine.base_layer.Layer)\n",
      " |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`). These are all attributes of\n",
      " |  `Dense`.\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`, then we create\n",
      " |  a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\n",
      " |  of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n",
      " |  `batch_size * d0` such sub-tensors).  The output in this case will have\n",
      " |  shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  When a popular kwarg `input_shape` is passed, then keras will create\n",
      " |  an input layer to insert before the current layer. This can be treated\n",
      " |  equivalent to explicitly defining an `InputLayer`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
      " |  >>> model = tf.keras.models.Sequential()\n",
      " |  >>> model.add(tf.keras.Input(shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
      " |  >>> # Now the model will take as input arrays of shape (None, 16)\n",
      " |  >>> # and output arrays of shape (None, 32).\n",
      " |  >>> # Note that after the first layer, you don't need to specify\n",
      " |  >>> # the size of the input anymore:\n",
      " |  >>> model.add(tf.keras.layers.Dense(32))\n",
      " |  >>> model.output_shape\n",
      " |  (None, 32)\n",
      " |  \n",
      " |  Args:\n",
      " |      units: Positive integer, dimensionality of the output space.\n",
      " |      activation: Activation function to use.\n",
      " |          If you don't specify anything, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |      kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |      bias_initializer: Initializer for the bias vector.\n",
      " |      kernel_regularizer: Regularizer function applied to\n",
      " |          the `kernel` weights matrix.\n",
      " |      bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |      activity_regularizer: Regularizer function applied to\n",
      " |          the output of the layer (its \"activation\").\n",
      " |      kernel_constraint: Constraint function applied to\n",
      " |          the `kernel` weights matrix.\n",
      " |      bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |      N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |      The most common situation would be\n",
      " |      a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |      N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |      For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |      the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      keras.src.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call. It is invoked automatically before\n",
      " |      the first execution of `call()`.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses\n",
      " |      (at the discretion of the subclass implementer).\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      The `call()` method may not create state (except in its first\n",
      " |      invocation, wrapping the creation of variables or other resources in\n",
      " |      `tf.init_scope()`).  It is recommended to create state, including\n",
      " |      `tf.Variable` instances and nested `Layer` instances,\n",
      " |       in `__init__()`, or in the `build()` method that is\n",
      " |      called automatically before `call()` executes for the first time.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as\n",
      " |            tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs`\n",
      " |            only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask\n",
      " |            generated for `inputs` by the previous layer (if `input` did come\n",
      " |            from a layer that generated a corresponding mask, i.e. if it came\n",
      " |            from a Keras layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      This method will cause the layer's state to be built, if that has not\n",
      " |      happened before. This requires that the layer will later be used with\n",
      " |      inputs that match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`,\n",
      " |              or structure of shape tuples / `tf.TensorShape` instances\n",
      " |              (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `tf.TensorShape` instance\n",
      " |          or structure of `tf.TensorShape` instances.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of\n",
      " |      dict every time it is called. The callers should make a copy of the\n",
      " |      returned dict if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific\n",
      " |          uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid\n",
      " |          value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the\n",
      " |          constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be\n",
      " |      dependent on the inputs passed when calling a layer. Hence, when reusing\n",
      " |      the same layer on different inputs `a` and `b`, some entries in\n",
      " |      `layer.losses` may be dependent on `a` and some on `b`. This method\n",
      " |      automatically keeps track of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      The same code works in distributed training: the input to `add_loss()`\n",
      " |      is treated like a regularization loss and averaged across replicas\n",
      " |      by the training loop (both built-in `Model.fit()` and compliant custom\n",
      " |      training loops).\n",
      " |      \n",
      " |      The `add_loss` method can also be called directly on a Functional Model\n",
      " |      during construction. In this case, any loss Tensors passed to this Model\n",
      " |      must be symbolic and be able to be traced back to the model's `Input`s.\n",
      " |      These losses become part of the model's topology and are tracked in\n",
      " |      `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss\n",
      " |      references a `Variable` of one of the model's layers), you can wrap your\n",
      " |      loss in a zero-argument lambda. These losses are not tracked as part of\n",
      " |      the model's topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n",
      " |          losses may also be zero-argument callables which create a loss\n",
      " |          tensor.\n",
      " |        **kwargs: Used for backwards compatibility only.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This\n",
      " |      is because we cannot trace the metric result tensor back to the model's\n",
      " |      inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result\n",
      " |          of calling a `keras.Metric` instance, it will be aggregated by\n",
      " |          default using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and\n",
      " |      variance in a BatchNormalization layer) may be dependent on the inputs\n",
      " |      passed when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case,\n",
      " |      variable updates are run on the fly and thus do not need to be tracked\n",
      " |      for later execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use a `ResourceVariable` or not.\n",
      " |          See [this guide](\n",
      " |          https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n",
      " |           for more information.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set\n",
      " |          to `AUTO` and the current `DistributionStrategy` chooses when to\n",
      " |          synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n",
      " |          must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as\n",
      " |          `ON_READ`.\n",
      " |  \n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |      \n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects,\n",
      " |          describing how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after\n",
      " |      updating a layer weights. It can be overridden to finalize any\n",
      " |      additional layer state after a weight update.\n",
      " |      \n",
      " |      This function will be called after weights of a layer have been restored\n",
      " |      from a loaded model.\n",
      " |  \n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |      \n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |      \n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with\n",
      " |      this layer as a list of NumPy arrays, which can in turn be used to load\n",
      " |      state into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
      " |      matrix and the bias vector. These can be used to set the weights of\n",
      " |      another `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
      " |      matrix and the bias vector. These can be used to set the weights of\n",
      " |      another `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which\n",
      " |      causes computations and the output to be in the compute dtype as well.\n",
      " |      This is done by the base Layer class in `Layer.__call__`, so you do not\n",
      " |      have to insert these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision\n",
      " |      when `compute_dtype` is float16 or bfloat16 for numeric stability. The\n",
      " |      output will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Return Functional API nodes upstream of this layer.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is\n",
      " |      accessed, so it is eager safe: accessing `losses` under a\n",
      " |      `tf.GradientTape` will propagate gradients back to the corresponding\n",
      " |      variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are\n",
      " |      expected to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Return Functional API nodes downstream of this layer.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are\n",
      " |      not themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to\n",
      " |      enable the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input\n",
      " |      tensor of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a\n",
      " |      nicely-formatted error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.layers.Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8c5e2b3-aa38-42ed-9618-501e3bf3b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting image values (x_...) to a 0..1 range.\n",
    "x_train = x_train / 255.0\n",
    "x_val = x_val / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "debf889c-927c-4671-9967-18d33bbfb19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n",
      "range of x_... images =  0.10146996  ->  0.96307296\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "print('range of x_... images = ', x_train[0].min(), ' -> ', x_train[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a694cf87-9c14-4ed5-b419-4c090507db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               393344    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 394634 (1.51 MB)\n",
      "Trainable params: 394634 (1.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Code the model.\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "98f06201-4ef1-417f-a8ab-f5367659eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "039b8ef6-6358-4d44-a1c1-6ee5a475bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 9s 5ms/step - loss: 2.0420 - accuracy: 0.2624 - val_loss: 1.9625 - val_accuracy: 0.2891\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.9306 - accuracy: 0.3025 - val_loss: 1.8701 - val_accuracy: 0.3268\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8807 - accuracy: 0.3211 - val_loss: 1.8751 - val_accuracy: 0.3250\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.8534 - accuracy: 0.3327 - val_loss: 1.8252 - val_accuracy: 0.3445\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.8324 - accuracy: 0.3404 - val_loss: 1.8358 - val_accuracy: 0.3402\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.8107 - accuracy: 0.3496 - val_loss: 1.8123 - val_accuracy: 0.3463\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.7984 - accuracy: 0.3553 - val_loss: 1.8616 - val_accuracy: 0.3214\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7855 - accuracy: 0.3560 - val_loss: 1.8013 - val_accuracy: 0.3495\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.7801 - accuracy: 0.3596 - val_loss: 1.7905 - val_accuracy: 0.3555\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7698 - accuracy: 0.3622 - val_loss: 1.8082 - val_accuracy: 0.3430\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, verbose=1, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "73060ccc-242c-44b5-a455-0429d7c7c29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot all relavant data.\n",
    "def plot_data(history):\n",
    "    epochs = range(1, 1 + len(history.history['loss']))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(epochs, history.history['loss'], 'r', label='train_loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'b', label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(epochs, history.history['accuracy'], 'r', label='acc')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'b', label='val_acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d7620e3f-bb3c-4d79-9f8c-2f10be619614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                196672    \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 197322 (770.79 KB)\n",
      "Trainable params: 197322 (770.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# After added some regularization.\n",
    "# Code the model.\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(keras.layers.Dense(10, activation='softmax', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f30a4fde-4330-4db8-b0bd-6d0feab127a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 3.8756 - accuracy: 0.2689 - val_loss: 3.0735 - val_accuracy: 0.3080\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.6968 - accuracy: 0.3077 - val_loss: 2.4237 - val_accuracy: 0.3006\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.2860 - accuracy: 0.3149 - val_loss: 2.2062 - val_accuracy: 0.3131\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1340 - accuracy: 0.3227 - val_loss: 2.0934 - val_accuracy: 0.3178\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0768 - accuracy: 0.3223 - val_loss: 2.0673 - val_accuracy: 0.3270\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0539 - accuracy: 0.3254 - val_loss: 2.0530 - val_accuracy: 0.3235\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0443 - accuracy: 0.3293 - val_loss: 2.0372 - val_accuracy: 0.3254\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0389 - accuracy: 0.3305 - val_loss: 2.0430 - val_accuracy: 0.3220\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0349 - accuracy: 0.3304 - val_loss: 2.0362 - val_accuracy: 0.3295\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0342 - accuracy: 0.3313 - val_loss: 2.0377 - val_accuracy: 0.3207\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0313 - accuracy: 0.3319 - val_loss: 2.0324 - val_accuracy: 0.3276\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0309 - accuracy: 0.3347 - val_loss: 2.0406 - val_accuracy: 0.3196\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0298 - accuracy: 0.3314 - val_loss: 2.0278 - val_accuracy: 0.3341\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0293 - accuracy: 0.3347 - val_loss: 2.0279 - val_accuracy: 0.3272\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 2.0289 - accuracy: 0.3352 - val_loss: 2.0295 - val_accuracy: 0.3219\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 2.0283 - accuracy: 0.3347 - val_loss: 2.0290 - val_accuracy: 0.3278\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0269 - accuracy: 0.3358 - val_loss: 2.0310 - val_accuracy: 0.3225\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0274 - accuracy: 0.3356 - val_loss: 2.0257 - val_accuracy: 0.3336\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0266 - accuracy: 0.3347 - val_loss: 2.0281 - val_accuracy: 0.3292\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0269 - accuracy: 0.3352 - val_loss: 2.0394 - val_accuracy: 0.3257\n"
     ]
    }
   ],
   "source": [
    "# Compile and train\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, verbose=1, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3dc7460-3404-4b6c-902d-a917c8d14dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6826254c-60bf-44ca-83ad-72253e22e302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "35002f08-288c-4bf1-bdda-f9a2a4990297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0192 - accuracy: 0.3396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.019176721572876, 0.33959999680519104]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7c52cd77-b7b1-4a1c-b466-a51918de264a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sequential in module keras.src.engine.sequential:\n",
      "\n",
      "class Sequential(keras.src.engine.functional.Functional)\n",
      " |  Sequential(layers=None, name=None)\n",
      " |  \n",
      " |  `Sequential` groups a linear stack of layers into a `tf.keras.Model`.\n",
      " |  \n",
      " |  `Sequential` provides training and inference features on this model.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Optionally, the first layer can receive an `input_shape` argument:\n",
      " |  model = tf.keras.Sequential()\n",
      " |  model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n",
      " |  # Afterwards, we do automatic shape inference:\n",
      " |  model.add(tf.keras.layers.Dense(4))\n",
      " |  \n",
      " |  # This is identical to the following:\n",
      " |  model = tf.keras.Sequential()\n",
      " |  model.add(tf.keras.Input(shape=(16,)))\n",
      " |  model.add(tf.keras.layers.Dense(8))\n",
      " |  \n",
      " |  # Note that you can also omit the `input_shape` argument.\n",
      " |  # In that case the model doesn't have any weights until the first call\n",
      " |  # to a training/evaluation method (since it isn't yet built):\n",
      " |  model = tf.keras.Sequential()\n",
      " |  model.add(tf.keras.layers.Dense(8))\n",
      " |  model.add(tf.keras.layers.Dense(4))\n",
      " |  # model.weights not created yet\n",
      " |  \n",
      " |  # Whereas if you specify the input shape, the model gets built\n",
      " |  # continuously as you are adding layers:\n",
      " |  model = tf.keras.Sequential()\n",
      " |  model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n",
      " |  model.add(tf.keras.layers.Dense(4))\n",
      " |  len(model.weights)\n",
      " |  # Returns \"4\"\n",
      " |  \n",
      " |  # When using the delayed-build pattern (no input shape specified), you can\n",
      " |  # choose to manually build your model by calling\n",
      " |  # `build(batch_input_shape)`:\n",
      " |  model = tf.keras.Sequential()\n",
      " |  model.add(tf.keras.layers.Dense(8))\n",
      " |  model.add(tf.keras.layers.Dense(4))\n",
      " |  model.build((None, 16))\n",
      " |  len(model.weights)\n",
      " |  # Returns \"4\"\n",
      " |  \n",
      " |  # Note that when using the delayed-build pattern (no input shape specified),\n",
      " |  # the model gets built the first time you call `fit`, `eval`, or `predict`,\n",
      " |  # or the first time you call the model on some input data.\n",
      " |  model = tf.keras.Sequential()\n",
      " |  model.add(tf.keras.layers.Dense(8))\n",
      " |  model.add(tf.keras.layers.Dense(1))\n",
      " |  model.compile(optimizer='sgd', loss='mse')\n",
      " |  # This builds the model for the first time:\n",
      " |  model.fit(x, y, batch_size=32, epochs=10)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      keras.src.engine.functional.Functional\n",
      " |      keras.src.engine.training.Model\n",
      " |      keras.src.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.utils.version_utils.LayerVersionSelector\n",
      " |      keras.src.utils.version_utils.ModelVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, layers=None, name=None)\n",
      " |      Creates a `Sequential` model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        layers: Optional list of layers to add to the model.\n",
      " |        name: Optional name for the model.\n",
      " |  \n",
      " |  add(self, layer)\n",
      " |      Adds a layer instance on top of the layer stack.\n",
      " |      \n",
      " |      Args:\n",
      " |          layer: layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If `layer` is not a layer instance.\n",
      " |          ValueError: In case the `layer` argument does not\n",
      " |              know its input shape.\n",
      " |          ValueError: In case the `layer` argument has\n",
      " |              multiple output tensors, or is already connected\n",
      " |              somewhere else (forbidden in `Sequential` models).\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |      Builds the model based on input shapes received.\n",
      " |      \n",
      " |      This is to be used for subclassed models, which do not know at\n",
      " |      instantiation time what their inputs look like.\n",
      " |      \n",
      " |      This method only exists for users who want to call `model.build()` in a\n",
      " |      standalone way (as a substitute for calling the model on real data to\n",
      " |      build it). It will never be called by the framework (and thus it will\n",
      " |      never throw unexpected errors in an unrelated workflow).\n",
      " |      \n",
      " |      Args:\n",
      " |       input_shape: Single tuple, `TensorShape` instance, or list/dict of\n",
      " |         shapes, where shapes are tuples, integers, or `TensorShape`\n",
      " |         instances.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError:\n",
      " |          1. In case of invalid user-provided data (not of type tuple,\n",
      " |             list, `TensorShape`, or dict).\n",
      " |          2. If the model requires call arguments that are agnostic\n",
      " |             to the input shapes (positional or keyword arg in call\n",
      " |             signature).\n",
      " |          3. If not all layers were properly built.\n",
      " |          4. If float type inputs are not supported within the layers.\n",
      " |      \n",
      " |        In each of these cases, the user should build their model by calling\n",
      " |        it on real tensor data.\n",
      " |  \n",
      " |  call(self, inputs, training=None, mask=None)\n",
      " |      Calls the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          training: Boolean or boolean scalar tensor, indicating whether to\n",
      " |              run the `Network` in training mode or inference mode.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      This method will cause the layer's state to be built, if that has not\n",
      " |      happened before. This requires that the layer will later be used with\n",
      " |      inputs that match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`,\n",
      " |              or structure of shape tuples / `tf.TensorShape` instances\n",
      " |              (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `tf.TensorShape` instance\n",
      " |          or structure of `tf.TensorShape` instances.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the `Model`.\n",
      " |      \n",
      " |      Config is a Python dictionary (serializable) containing the\n",
      " |      configuration of an object, which in this case is a `Model`. This allows\n",
      " |      the `Model` to be be reinstantiated later (without its trained weights)\n",
      " |      from this configuration.\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of\n",
      " |      dict every time it is called. The callers should make a copy of the\n",
      " |      returned dict if they want to modify it.\n",
      " |      \n",
      " |      Developers of subclassed `Model` are advised to override this method,\n",
      " |      and continue to update the dict from `super(MyModel, self).get_config()`\n",
      " |      to provide the proper configuration of this `Model`. The default config\n",
      " |      will return config dict for init parameters if they are basic types.\n",
      " |      Raises `NotImplementedError` when in cases where a custom\n",
      " |      `get_config()` implementation is required for the subclassed model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary containing the configuration of this `Model`.\n",
      " |  \n",
      " |  pop(self)\n",
      " |      Removes the last layer in the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: if there are no layers in the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to\n",
      " |      enable the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input\n",
      " |      tensor of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a\n",
      " |      nicely-formatted error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.engine.functional.Functional:\n",
      " |  \n",
      " |  get_weight_paths(self)\n",
      " |      Retrieve all the variables and their paths for the model.\n",
      " |      \n",
      " |      The variable path (string) is a stable key to identify a `tf.Variable`\n",
      " |      instance owned by the model. It can be used to specify variable-specific\n",
      " |      configurations (e.g. DTensor, quantization) from a global view.\n",
      " |      \n",
      " |      This method returns a dict with weight object paths as keys\n",
      " |      and the corresponding `tf.Variable` instances as values.\n",
      " |      \n",
      " |      Note that if the model is a subclassed model and the weights haven't\n",
      " |      been initialized, an empty dict will be returned.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict where keys are variable paths and values are `tf.Variable`\n",
      " |           instances.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class SubclassModel(tf.keras.Model):\n",
      " |      \n",
      " |        def __init__(self, name=None):\n",
      " |          super().__init__(name=name)\n",
      " |          self.d1 = tf.keras.layers.Dense(10)\n",
      " |          self.d2 = tf.keras.layers.Dense(20)\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          x = self.d1(inputs)\n",
      " |          return self.d2(x)\n",
      " |      \n",
      " |      model = SubclassModel()\n",
      " |      model(tf.zeros((10, 10)))\n",
      " |      weight_paths = model.get_weight_paths()\n",
      " |      # weight_paths:\n",
      " |      # {\n",
      " |      #    'd1.kernel': model.d1.kernel,\n",
      " |      #    'd1.bias': model.d1.bias,\n",
      " |      #    'd2.kernel': model.d2.kernel,\n",
      " |      #    'd2.bias': model.d2.bias,\n",
      " |      # }\n",
      " |      \n",
      " |      # Functional model\n",
      " |      inputs = tf.keras.Input((10,), batch_size=10)\n",
      " |      x = tf.keras.layers.Dense(20, name='d1')(inputs)\n",
      " |      output = tf.keras.layers.Dense(30, name='d2')(x)\n",
      " |      model = tf.keras.Model(inputs, output)\n",
      " |      d1 = model.layers[1]\n",
      " |      d2 = model.layers[2]\n",
      " |      weight_paths = model.get_weight_paths()\n",
      " |      # weight_paths:\n",
      " |      # {\n",
      " |      #    'd1.kernel': d1.kernel,\n",
      " |      #    'd1.bias': d1.bias,\n",
      " |      #    'd2.kernel': d2.kernel,\n",
      " |      #    'd2.bias': d2.bias,\n",
      " |      # }\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.engine.functional.Functional:\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.engine.training.Model:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, jit_compile=None, pss_evaluation_shards=0, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
      " |                    loss=tf.keras.losses.BinaryCrossentropy(),\n",
      " |                    metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
      " |                             tf.keras.metrics.FalseNegatives()])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
      " |            `tf.keras.optimizers`.\n",
      " |          loss: Loss function. May be a string (name of loss function), or\n",
      " |            a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n",
      " |            function is any callable with the signature `loss = fn(y_true,\n",
      " |            y_pred)`, where `y_true` are the ground truth values, and\n",
      " |            `y_pred` are the model's predictions.\n",
      " |            `y_true` should have shape\n",
      " |            `(batch_size, d0, .. dN)` (except in the case of\n",
      " |            sparse loss functions such as\n",
      " |            sparse categorical crossentropy which expects integer arrays of\n",
      " |            shape `(batch_size, d0, .. dN-1)`).\n",
      " |            `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
      " |            The loss function should return a float tensor.\n",
      " |            If a custom `Loss` instance is\n",
      " |            used and reduction is set to `None`, return value has shape\n",
      " |            `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n",
      " |            values; otherwise, it is a scalar. If the model has multiple\n",
      " |            outputs, you can use a different loss on each output by passing a\n",
      " |            dictionary or a list of losses. The loss value that will be\n",
      " |            minimized by the model will then be the sum of all individual\n",
      " |            losses, unless `loss_weights` is specified.\n",
      " |          metrics: List of metrics to be evaluated by the model during\n",
      " |            training and testing. Each of this can be a string (name of a\n",
      " |            built-in function), function or a `tf.keras.metrics.Metric`\n",
      " |            instance. See `tf.keras.metrics`. Typically you will use\n",
      " |            `metrics=['accuracy']`.\n",
      " |            A function is any callable with the signature `result = fn(y_true,\n",
      " |            y_pred)`. To specify different metrics for different outputs of a\n",
      " |            multi-output model, you could also pass a dictionary, such as\n",
      " |            `metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']}`.\n",
      " |            You can also pass a list to specify a metric or a list of metrics\n",
      " |            for each output, such as\n",
      " |            `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
      " |            or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
      " |            strings 'accuracy' or 'acc', we convert this to one of\n",
      " |            `tf.keras.metrics.BinaryAccuracy`,\n",
      " |            `tf.keras.metrics.CategoricalAccuracy`,\n",
      " |            `tf.keras.metrics.SparseCategoricalAccuracy` based on the shapes\n",
      " |            of the targets and of the model output. We do a similar\n",
      " |            conversion for the strings 'crossentropy' and 'ce' as well.\n",
      " |            The metrics passed here are evaluated without sample weighting; if\n",
      " |            you would like sample weighting to apply, you can specify your\n",
      " |            metrics via the `weighted_metrics` argument instead.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |            coefficients (Python floats) to weight the loss contributions of\n",
      " |            different model outputs. The loss value that will be minimized by\n",
      " |            the model will then be the *weighted sum* of all individual\n",
      " |            losses, weighted by the `loss_weights` coefficients.  If a list,\n",
      " |            it is expected to have a 1:1 mapping to the model's outputs. If a\n",
      " |            dict, it is expected to map output names (strings) to scalar\n",
      " |            coefficients.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
      " |            `sample_weight` or `class_weight` during training and testing.\n",
      " |          run_eagerly: Bool. If `True`, this `Model`'s logic will not be\n",
      " |            wrapped in a `tf.function`. Recommended to leave this as `None`\n",
      " |            unless your `Model` cannot be run inside a `tf.function`.\n",
      " |            `run_eagerly=True` is not supported when using\n",
      " |            `tf.distribute.experimental.ParameterServerStrategy`. Defaults to\n",
      " |             `False`.\n",
      " |          steps_per_execution: Int. The number of batches to\n",
      " |            run during each `tf.function` call. Running multiple batches\n",
      " |            inside a single `tf.function` call can greatly improve performance\n",
      " |            on TPUs or small models with a large Python overhead. At most, one\n",
      " |            full epoch will be run each execution. If a number larger than the\n",
      " |            size of the epoch is passed, the execution will be truncated to\n",
      " |            the size of the epoch. Note that if `steps_per_execution` is set\n",
      " |            to `N`, `Callback.on_batch_begin` and `Callback.on_batch_end`\n",
      " |            methods will only be called every `N` batches (i.e. before/after\n",
      " |            each `tf.function` execution). Defaults to `1`.\n",
      " |          jit_compile: If `True`, compile the model training step with XLA.\n",
      " |            [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n",
      " |            for machine learning.\n",
      " |            `jit_compile` is not enabled for by default.\n",
      " |            Note that `jit_compile=True`\n",
      " |            may not necessarily work for all models.\n",
      " |            For more information on supported operations please refer to the\n",
      " |            [XLA documentation](https://www.tensorflow.org/xla).\n",
      " |            Also refer to\n",
      " |            [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n",
      " |            for more details.\n",
      " |          pss_evaluation_shards: Integer or 'auto'. Used for\n",
      " |            `tf.distribute.ParameterServerStrategy` training only. This arg\n",
      " |            sets the number of shards to split the dataset into, to enable an\n",
      " |            exact visitation guarantee for evaluation, meaning the model will\n",
      " |            be applied to each dataset element exactly once, even if workers\n",
      " |            fail. The dataset must be sharded to ensure separate workers do\n",
      " |            not process the same data. The number of shards should be at least\n",
      " |            the number of workers for good performance. A value of 'auto'\n",
      " |            turns on exact evaluation and uses a heuristic for the number of\n",
      " |            shards based on the number of workers. 0, meaning no\n",
      " |            visitation guarantee is provided. NOTE: Custom implementations of\n",
      " |            `Model.test_step` will be ignored when doing exact evaluation.\n",
      " |            Defaults to `0`.\n",
      " |          **kwargs: Arguments supported for backwards compatibility only.\n",
      " |  \n",
      " |  compile_from_config(self, config)\n",
      " |      Compiles the model with the information given in config.\n",
      " |      \n",
      " |      This method uses the information in the config (optimizer, loss,\n",
      " |      metrics, etc.) to compile the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing information for compiling the model.\n",
      " |  \n",
      " |  compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None)\n",
      " |      Compute the total loss, validate it, and return it.\n",
      " |      \n",
      " |      Subclasses can optionally override this method to provide custom loss\n",
      " |      computation logic.\n",
      " |      \n",
      " |      Example:\n",
      " |      ```python\n",
      " |      class MyModel(tf.keras.Model):\n",
      " |      \n",
      " |        def __init__(self, *args, **kwargs):\n",
      " |          super(MyModel, self).__init__(*args, **kwargs)\n",
      " |          self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
      " |      \n",
      " |        def compute_loss(self, x, y, y_pred, sample_weight):\n",
      " |          loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y))\n",
      " |          loss += tf.add_n(self.losses)\n",
      " |          self.loss_tracker.update_state(loss)\n",
      " |          return loss\n",
      " |      \n",
      " |        def reset_metrics(self):\n",
      " |          self.loss_tracker.reset_states()\n",
      " |      \n",
      " |        @property\n",
      " |        def metrics(self):\n",
      " |          return [self.loss_tracker]\n",
      " |      \n",
      " |      tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,))\n",
      " |      dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1)\n",
      " |      \n",
      " |      inputs = tf.keras.layers.Input(shape=(10,), name='my_input')\n",
      " |      outputs = tf.keras.layers.Dense(10)(inputs)\n",
      " |      model = MyModel(inputs, outputs)\n",
      " |      model.add_loss(tf.reduce_sum(outputs))\n",
      " |      \n",
      " |      optimizer = tf.keras.optimizers.SGD()\n",
      " |      model.compile(optimizer, loss='mse', steps_per_execution=10)\n",
      " |      model.fit(dataset, epochs=2, steps_per_epoch=10)\n",
      " |      print('My custom loss: ', model.loss_tracker.result().numpy())\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Input data.\n",
      " |        y: Target data.\n",
      " |        y_pred: Predictions returned by the model (output of `model(x)`)\n",
      " |        sample_weight: Sample weights for weighting the loss function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The total loss as a `tf.Tensor`, or `None` if no loss results (which\n",
      " |        is the case when called by `Model.test_step`).\n",
      " |  \n",
      " |  compute_metrics(self, x, y, y_pred, sample_weight)\n",
      " |      Update metric states and collect all metrics to be returned.\n",
      " |      \n",
      " |      Subclasses can optionally override this method to provide custom metric\n",
      " |      updating and collection logic.\n",
      " |      \n",
      " |      Example:\n",
      " |      ```python\n",
      " |      class MyModel(tf.keras.Sequential):\n",
      " |      \n",
      " |        def compute_metrics(self, x, y, y_pred, sample_weight):\n",
      " |      \n",
      " |          # This super call updates `self.compiled_metrics` and returns\n",
      " |          # results for all metrics listed in `self.metrics`.\n",
      " |          metric_results = super(MyModel, self).compute_metrics(\n",
      " |              x, y, y_pred, sample_weight)\n",
      " |      \n",
      " |          # Note that `self.custom_metric` is not listed in `self.metrics`.\n",
      " |          self.custom_metric.update_state(x, y, y_pred, sample_weight)\n",
      " |          metric_results['custom_metric_name'] = self.custom_metric.result()\n",
      " |          return metric_results\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Input data.\n",
      " |        y: Target data.\n",
      " |        y_pred: Predictions returned by the model (output of `model.call(x)`)\n",
      " |        sample_weight: Sample weights for weighting the loss function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `dict` containing values that will be passed to\n",
      " |        `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the\n",
      " |        values of the metrics listed in `self.metrics` are returned. Example:\n",
      " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose='auto', sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False, **kwargs)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches (see the `batch_size` arg.)\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset. Should return a tuple\n",
      " |              of either `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |            - A generator or `keras.utils.Sequence` returning `(inputs,\n",
      " |              targets)` or `(inputs, targets, sample_weights)`.\n",
      " |            A more detailed description of unpacking behavior for iterator\n",
      " |            types (Dataset, generator, Sequence) is given in the `Unpacking\n",
      " |            behavior for iterator-like inputs` section of `Model.fit`.\n",
      " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
      " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
      " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
      " |            If `x` is a dataset, generator or `keras.utils.Sequence` instance,\n",
      " |            `y` should not be specified (since targets will be obtained from\n",
      " |            the iterator/dataset).\n",
      " |          batch_size: Integer or `None`. Number of samples per batch of\n",
      " |            computation. If unspecified, `batch_size` will default to 32. Do\n",
      " |            not specify the `batch_size` if your data is in the form of a\n",
      " |            dataset, generators, or `keras.utils.Sequence` instances (since\n",
      " |            they generate batches).\n",
      " |          verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = single line.\n",
      " |              `\"auto\"` becomes 1 for most cases, and to 2 when used with\n",
      " |              `ParameterServerStrategy`. Note that the progress bar is not\n",
      " |              particularly useful when logged to a file, so `verbose=2` is\n",
      " |              recommended when not running interactively (e.g. in a production\n",
      " |              environment). Defaults to 'auto'.\n",
      " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
      " |            used for weighting the loss function. You can either pass a flat\n",
      " |            (1D) Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples), or in the case of\n",
      " |                temporal data, you can pass a 2D array with shape `(samples,\n",
      " |                sequence_length)`, to apply a different weight to every\n",
      " |                timestep of every sample. This argument is not supported when\n",
      " |                `x` is a dataset, instead pass sample weights as the third\n",
      " |                element of `x`.\n",
      " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
      " |            before declaring the evaluation round finished. Ignored with the\n",
      " |            default value of `None`. If x is a `tf.data` dataset and `steps`\n",
      " |            is None, 'evaluate' will run until the dataset is exhausted. This\n",
      " |            argument is not supported with array inputs.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
      " |            callbacks to apply during evaluation. See\n",
      " |            [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Integer. Used for generator or\n",
      " |            `keras.utils.Sequence` input only. Maximum size for the generator\n",
      " |            queue. If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |            only. Maximum number of processes to spin up when using\n",
      " |            process-based threading. If unspecified, `workers` will default to\n",
      " |            1.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |            threading. If unspecified, `use_multiprocessing` will default to\n",
      " |            `False`. Note that because this implementation relies on\n",
      " |            multiprocessing, you should not pass non-picklable arguments to\n",
      " |            the generator as they can't be passed easily to children\n",
      " |            processes.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |            dict, with each key being the name of the metric. If `False`, they\n",
      " |            are returned as a list.\n",
      " |          **kwargs: Unused at this time.\n",
      " |      \n",
      " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
      " |      `Model.fit`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If `model.evaluate` is wrapped in a `tf.function`.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Evaluates the model on a data generator.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.evaluate` now supports generators, so there is no longer any\n",
      " |        need to use this endpoint.\n",
      " |  \n",
      " |  export(self, filepath)\n",
      " |      Create a SavedModel artifact for inference (e.g. via TF-Serving).\n",
      " |      \n",
      " |      This method lets you export a model to a lightweight SavedModel artifact\n",
      " |      that contains the model's forward pass only (its `call()` method)\n",
      " |      and can be served via e.g. TF-Serving. The forward pass is registered\n",
      " |      under the name `serve()` (see example below).\n",
      " |      \n",
      " |      The original code of the model (including any custom layers you may\n",
      " |      have used) is *no longer* necessary to reload the artifact -- it is\n",
      " |      entirely standalone.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: `str` or `pathlib.Path` object. Path where to save\n",
      " |              the artifact.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Create the artifact\n",
      " |      model.export(\"path/to/location\")\n",
      " |      \n",
      " |      # Later, in a different process / environment...\n",
      " |      reloaded_artifact = tf.saved_model.load(\"path/to/location\")\n",
      " |      predictions = reloaded_artifact.serve(input_data)\n",
      " |      ```\n",
      " |      \n",
      " |      If you would like to customize your serving endpoints, you can\n",
      " |      use the lower-level `keras.export.ExportArchive` class. The `export()`\n",
      " |      method relies on `ExportArchive` internally.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose='auto', callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Trains the model for a fixed number of epochs (dataset iterations).\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset. Should return a tuple\n",
      " |              of either `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |            - A generator or `keras.utils.Sequence` returning `(inputs,\n",
      " |              targets)` or `(inputs, targets, sample_weights)`.\n",
      " |            - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
      " |              callable that takes a single argument of type\n",
      " |              `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
      " |              `DatasetCreator` should be used when users prefer to specify the\n",
      " |              per-replica batching and sharding logic for the `Dataset`.\n",
      " |              See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
      " |              information.\n",
      " |            A more detailed description of unpacking behavior for iterator\n",
      " |            types (Dataset, generator, Sequence) is given below. If these\n",
      " |            include `sample_weights` as a third component, note that sample\n",
      " |            weighting applies to the `weighted_metrics` argument but not the\n",
      " |            `metrics` argument in `compile()`. If using\n",
      " |            `tf.distribute.experimental.ParameterServerStrategy`, only\n",
      " |            `DatasetCreator` type is supported for `x`.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
      " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
      " |            or `keras.utils.Sequence` instance, `y` should\n",
      " |            not be specified (since targets will be obtained from `x`).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of datasets, generators, or `keras.utils.Sequence`\n",
      " |              instances (since they generate batches).\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided\n",
      " |              (unless the `steps_per_epoch` flag is set to\n",
      " |              something other than None).\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |              'auto' becomes 1 for most cases, but 2 when used with\n",
      " |              `ParameterServerStrategy`. Note that the progress bar is not\n",
      " |              particularly useful when logged to a file, so verbose=2 is\n",
      " |              recommended when not running interactively (eg, in a production\n",
      " |              environment). Defaults to 'auto'.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See `tf.keras.callbacks`. Note\n",
      " |              `tf.keras.callbacks.ProgbarLogger` and\n",
      " |              `tf.keras.callbacks.History` callbacks are created automatically\n",
      " |              and need not be passed into `model.fit`.\n",
      " |              `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
      " |              `verbose` argument to `model.fit`.\n",
      " |              Callbacks with batch-level calls are currently unsupported with\n",
      " |              `tf.distribute.experimental.ParameterServerStrategy`, and users\n",
      " |              are advised to implement epoch-level calls instead with an\n",
      " |              appropriate `steps_per_epoch` value.\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling. This\n",
      " |              argument is not supported when `x` is a dataset, generator or\n",
      " |              `keras.utils.Sequence` instance.\n",
      " |              If both `validation_data` and `validation_split` are provided,\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |              `validation_split` is not yet supported with\n",
      " |              `tf.distribute.experimental.ParameterServerStrategy`.\n",
      " |          validation_data: Data on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data. Thus, note the fact\n",
      " |              that the validation loss of data provided using\n",
      " |              `validation_split` or `validation_data` is not affected by\n",
      " |              regularization layers like noise and dropout.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |              `validation_data` could be:\n",
      " |                - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
      " |                - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n",
      " |                  arrays.\n",
      " |                - A `tf.data.Dataset`.\n",
      " |                - A Python generator or `keras.utils.Sequence` returning\n",
      " |                `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
      " |              `validation_data` is not yet supported with\n",
      " |              `tf.distribute.experimental.ParameterServerStrategy`.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch'). This argument is\n",
      " |              ignored when `x` is a generator or an object of tf.data.Dataset.\n",
      " |              'batch' is a special option for dealing\n",
      " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
      " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class. When `class_weight` is specified\n",
      " |              and targets have a rank of 2 or greater, either `y` must be\n",
      " |              one-hot encoded, or an explicit final dimension of `1` must\n",
      " |              be included for sparse class labels.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              This argument is not supported when `x` is a dataset, generator,\n",
      " |              or `keras.utils.Sequence` instance, instead provide the\n",
      " |              sample_weights as the third element of `x`.\n",
      " |              Note that sample weighting does not apply to metrics specified\n",
      " |              via the `metrics` argument in `compile()`. To apply sample\n",
      " |              weighting to your metrics, you can specify them via the\n",
      " |              `weighted_metrics` in `compile()` instead.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined. If x is a\n",
      " |              `tf.data` dataset, and 'steps_per_epoch'\n",
      " |              is None, the epoch will run until the input dataset is\n",
      " |              exhausted.  When passing an infinitely repeating dataset, you\n",
      " |              must specify the `steps_per_epoch` argument. If\n",
      " |              `steps_per_epoch=-1` the training will run indefinitely with an\n",
      " |              infinitely repeating dataset.  This argument is not supported\n",
      " |              with array inputs.\n",
      " |              When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
      " |                * `steps_per_epoch=None` is not supported.\n",
      " |          validation_steps: Only relevant if `validation_data` is provided and\n",
      " |              is a `tf.data` dataset. Total number of steps (batches of\n",
      " |              samples) to draw before stopping when performing validation\n",
      " |              at the end of every epoch. If 'validation_steps' is None,\n",
      " |              validation will run until the `validation_data` dataset is\n",
      " |              exhausted. In the case of an infinitely repeated dataset, it\n",
      " |              will run into an infinite loop. If 'validation_steps' is\n",
      " |              specified and only part of the dataset will be consumed, the\n",
      " |              evaluation will start from the beginning of the dataset at each\n",
      " |              epoch. This ensures that the same validation samples are used\n",
      " |              every time.\n",
      " |          validation_batch_size: Integer or `None`.\n",
      " |              Number of samples per validation batch.\n",
      " |              If unspecified, will default to `batch_size`.\n",
      " |              Do not specify the `validation_batch_size` if your data is in\n",
      " |              the form of datasets, generators, or `keras.utils.Sequence`\n",
      " |              instances (since they generate batches).\n",
      " |          validation_freq: Only relevant if validation data is provided.\n",
      " |            Integer or `collections.abc.Container` instance (e.g. list, tuple,\n",
      " |            etc.).  If an integer, specifies how many training epochs to run\n",
      " |            before a new validation run is performed, e.g. `validation_freq=2`\n",
      " |            runs validation every 2 epochs. If a Container, specifies the\n",
      " |            epochs on which to run validation, e.g.\n",
      " |            `validation_freq=[1, 2, 10]` runs validation at the end of the\n",
      " |            1st, 2nd, and 10th epochs.\n",
      " |          max_queue_size: Integer. Used for generator or\n",
      " |            `keras.utils.Sequence` input only. Maximum size for the generator\n",
      " |            queue.  If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up\n",
      " |              when using process-based threading. If unspecified, `workers`\n",
      " |              will default to 1.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children\n",
      " |              processes.\n",
      " |      \n",
      " |      Unpacking behavior for iterator-like inputs:\n",
      " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
      " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      " |        yield not only features (x) but optionally targets (y) and sample\n",
      " |        weights.  Keras requires that the output of such iterator-likes be\n",
      " |        unambiguous. The iterator should return a tuple of length 1, 2, or 3,\n",
      " |        where the optional second and third elements will be used for y and\n",
      " |        sample_weight respectively. Any other type provided will be wrapped in\n",
      " |        a length one tuple, effectively treating everything as 'x'. When\n",
      " |        yielding dicts, they should still adhere to the top-level tuple\n",
      " |        structure.\n",
      " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      " |        features, targets, and weights from the keys of a single dict.\n",
      " |          A notable unsupported data type is the namedtuple. The reason is\n",
      " |        that it behaves like both an ordered datatype (tuple) and a mapping\n",
      " |        datatype (dict). So given a namedtuple of the form:\n",
      " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      " |        it is ambiguous whether to reverse the order of the elements when\n",
      " |        interpreting the value. Even worse is a tuple of the form:\n",
      " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      " |        where it is unclear if the tuple was intended to be unpacked into x,\n",
      " |        y, and sample_weight or passed through as a single element to `x`. As\n",
      " |        a result the data processing code will simply raise a ValueError if it\n",
      " |        encounters a namedtuple. (Along with instructions to remedy the\n",
      " |        issue.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: 1. If the model was never compiled or,\n",
      " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
      " |      \n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects or when the input data is empty.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.fit` now supports generators, so there is no longer any need to\n",
      " |        use this endpoint.\n",
      " |  \n",
      " |  get_compile_config(self)\n",
      " |      Returns a serialized config with information for compiling the model.\n",
      " |      \n",
      " |      This method returns a config dictionary containing all the information\n",
      " |      (optimizer, loss, metrics, etc.) with which the model was compiled.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing information for compiling the model.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      Args:\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  get_metrics_result(self)\n",
      " |      Returns the model's metrics values as a dict.\n",
      " |      \n",
      " |      If any of the metric result is a dict (containing multiple metrics),\n",
      " |      each of them gets added to the top level returned dict of this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `dict` containing values of the metrics listed in `self.metrics`.\n",
      " |        Example:\n",
      " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A flat list of Numpy arrays.\n",
      " |  \n",
      " |  load_weights(self, filepath, skip_mismatch=False, by_name=False, options=None)\n",
      " |      Loads all layer weights from a saved files.\n",
      " |      \n",
      " |      The saved file could be a SavedModel file, a `.keras` file (v3 saving\n",
      " |      format), or a file created via `model.save_weights()`.\n",
      " |      \n",
      " |      By default, weights are loaded based on the network's\n",
      " |      topology. This means the architecture should be the same as when the\n",
      " |      weights were saved. Note that layers that don't have weights are not\n",
      " |      taken into account in the topological ordering, so adding or removing\n",
      " |      layers is fine as long as they don't have weights.\n",
      " |      \n",
      " |      **Partial weight loading**\n",
      " |      \n",
      " |      If you have modified your model, for instance by adding a new layer\n",
      " |      (with weights) or by changing the shape of the weights of a layer,\n",
      " |      you can choose to ignore errors and continue loading\n",
      " |      by setting `skip_mismatch=True`. In this case any layer with\n",
      " |      mismatching weights will be skipped. A warning will be displayed\n",
      " |      for each skipped layer.\n",
      " |      \n",
      " |      **Weight loading by name**\n",
      " |      \n",
      " |      If your weights are saved as a `.h5` file created\n",
      " |      via `model.save_weights()`, you can use the argument `by_name=True`.\n",
      " |      \n",
      " |      In this case, weights are loaded into layers only if they share\n",
      " |      the same name. This is useful for fine-tuning or transfer-learning\n",
      " |      models where some of the layers have changed.\n",
      " |      \n",
      " |      Note that only topological loading (`by_name=False`) is supported when\n",
      " |      loading weights from the `.keras` v3 format or from the TensorFlow\n",
      " |      SavedModel format.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: String, path to the weights file to load. For weight files\n",
      " |              in TensorFlow format, this is the file prefix (the same as was\n",
      " |              passed to `save_weights()`). This can also be a path to a\n",
      " |              SavedModel or a `.keras` file (v3 saving format) saved\n",
      " |              via `model.save()`.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers where\n",
      " |              there is a mismatch in the number of weights, or a mismatch in\n",
      " |              the shape of the weights.\n",
      " |          by_name: Boolean, whether to load weights by name or by topological\n",
      " |              order. Only topological loading is supported for weight files in\n",
      " |              the `.keras` v3 format or in the TensorFlow SavedModel format.\n",
      " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
      " |              options for loading weights (only valid for a SavedModel file).\n",
      " |  \n",
      " |  make_predict_function(self, force=False)\n",
      " |      Creates a function that executes one step of inference.\n",
      " |      \n",
      " |      This method can be overridden to support custom inference logic.\n",
      " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
      " |      \n",
      " |      Typically, this method directly controls `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
      " |      logic to `Model.predict_step`.\n",
      " |      \n",
      " |      This function is cached the first time `Model.predict` or\n",
      " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
      " |      `Model.compile` is called. You can skip the cache and generate again the\n",
      " |      function with `force=True`.\n",
      " |      \n",
      " |      Args:\n",
      " |        force: Whether to regenerate the predict function and skip the cached\n",
      " |          function if available.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Function. The function created by this method should accept a\n",
      " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
      " |  \n",
      " |  make_test_function(self, force=False)\n",
      " |      Creates a function that executes one step of evaluation.\n",
      " |      \n",
      " |      This method can be overridden to support custom evaluation logic.\n",
      " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
      " |      \n",
      " |      Typically, this method directly controls `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
      " |      logic to `Model.test_step`.\n",
      " |      \n",
      " |      This function is cached the first time `Model.evaluate` or\n",
      " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
      " |      `Model.compile` is called. You can skip the cache and generate again the\n",
      " |      function with `force=True`.\n",
      " |      \n",
      " |      Args:\n",
      " |        force: Whether to regenerate the test function and skip the cached\n",
      " |          function if available.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Function. The function created by this method should accept a\n",
      " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
      " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
      " |  \n",
      " |  make_train_function(self, force=False)\n",
      " |      Creates a function that executes one step of training.\n",
      " |      \n",
      " |      This method can be overridden to support custom training logic.\n",
      " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
      " |      \n",
      " |      Typically, this method directly controls `tf.function` and\n",
      " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
      " |      logic to `Model.train_step`.\n",
      " |      \n",
      " |      This function is cached the first time `Model.fit` or\n",
      " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
      " |      `Model.compile` is called. You can skip the cache and generate again the\n",
      " |      function with `force=True`.\n",
      " |      \n",
      " |      Args:\n",
      " |        force: Whether to regenerate the train function and skip the cached\n",
      " |          function if available.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Function. The function created by this method should accept a\n",
      " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
      " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
      " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose='auto', steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches. This method is designed for batch\n",
      " |      processing of large numbers of inputs. It is not intended for use inside\n",
      " |      of loops that iterate over your data and process small numbers of inputs\n",
      " |      at a time.\n",
      " |      \n",
      " |      For small numbers of inputs that fit in one batch,\n",
      " |      directly use `__call__()` for faster execution, e.g.,\n",
      " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
      " |      `tf.keras.layers.BatchNormalization` that behave differently during\n",
      " |      inference. You may pair the individual model call with a `tf.function`\n",
      " |      for additional performance inside your inner loop.\n",
      " |      If you need access to numpy array values instead of tensors after your\n",
      " |      model call, you can use `tensor.numpy()` to get the numpy array value of\n",
      " |      an eager tensor.\n",
      " |      \n",
      " |      Also, note the fact that test loss is not affected by\n",
      " |      regularization layers like noise and dropout.\n",
      " |      \n",
      " |      Note: See [this FAQ entry](\n",
      " |      https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call)\n",
      " |      for more details about the difference between `Model` methods\n",
      " |      `predict()` and `__call__()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input samples. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A `tf.data` dataset.\n",
      " |            - A generator or `keras.utils.Sequence` instance.\n",
      " |            A more detailed description of unpacking behavior for iterator\n",
      " |            types (Dataset, generator, Sequence) is given in the `Unpacking\n",
      " |            behavior for iterator-like inputs` section of `Model.fit`.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per batch.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
      " |              (since they generate batches).\n",
      " |          verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = single line.\n",
      " |              `\"auto\"` becomes 1 for most cases, and to 2 when used with\n",
      " |              `ParameterServerStrategy`. Note that the progress bar is not\n",
      " |              particularly useful when logged to a file, so `verbose=2` is\n",
      " |              recommended when not running interactively (e.g. in a production\n",
      " |              environment). Defaults to 'auto'.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
      " |              dataset and `steps` is None, `predict()` will\n",
      " |              run until the input dataset is exhausted.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during prediction.\n",
      " |              See [callbacks](\n",
      " |              https://www.tensorflow.org/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Integer. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. Maximum size for the\n",
      " |              generator queue. If unspecified, `max_queue_size` will default\n",
      " |              to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up when using\n",
      " |              process-based threading. If unspecified, `workers` will default\n",
      " |              to 1.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children\n",
      " |              processes.\n",
      " |      \n",
      " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
      " |      `Model.fit`. Note that Model.predict uses the same interpretation rules\n",
      " |      as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for\n",
      " |      all three methods.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If `model.predict` is wrapped in a `tf.function`.\n",
      " |          ValueError: In case of mismatch between the provided\n",
      " |              input data and the model's expectations,\n",
      " |              or in case a stateful model receives a number of samples\n",
      " |              that is not a multiple of the batch size.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.predict` now supports generators, so there is no longer any\n",
      " |        need to use this endpoint.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays (in case the\n",
      " |                model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors (in case the model has\n",
      " |                multiple inputs).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If `model.predict_on_batch` is wrapped in a\n",
      " |            `tf.function`.\n",
      " |  \n",
      " |  predict_step(self, data)\n",
      " |      The logic for one inference step.\n",
      " |      \n",
      " |      This method can be overridden to support custom inference logic.\n",
      " |      This method is called by `Model.make_predict_function`.\n",
      " |      \n",
      " |      This method should contain the mathematical logic for one step of\n",
      " |      inference.  This typically includes the forward pass.\n",
      " |      \n",
      " |      Configuration details for *how* this logic is run (e.g. `tf.function`\n",
      " |      and `tf.distribute.Strategy` settings), should be left to\n",
      " |      `Model.make_predict_function`, which can also be overridden.\n",
      " |      \n",
      " |      Args:\n",
      " |        data: A nested structure of `Tensor`s.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The result of one inference step, typically the output of calling the\n",
      " |        `Model` on data.\n",
      " |  \n",
      " |  reset_metrics(self)\n",
      " |      Resets the state of all the metrics in the model.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
      " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
      " |      \n",
      " |      >>> x = np.random.random((2, 3))\n",
      " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
      " |      >>> _ = model.fit(x, y, verbose=0)\n",
      " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
      " |      \n",
      " |      >>> model.reset_metrics()\n",
      " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, save_format=None, **kwargs)\n",
      " |      Saves a model as a TensorFlow SavedModel or HDF5 file.\n",
      " |      \n",
      " |      See the [Serialization and Saving guide](\n",
      " |          https://keras.io/guides/serialization_and_saving/) for details.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: Keras model instance to be saved.\n",
      " |          filepath: `str` or `pathlib.Path` object. Path where to save the\n",
      " |              model.\n",
      " |          overwrite: Whether we should overwrite any existing model at the\n",
      " |              target location, or instead ask the user via an interactive\n",
      " |              prompt.\n",
      " |          save_format: Either `\"keras\"`, `\"tf\"`, `\"h5\"`,\n",
      " |              indicating whether to save the model\n",
      " |              in the native Keras format (`.keras`),\n",
      " |              in the TensorFlow SavedModel format\n",
      " |              (referred to as \"SavedModel\" below),\n",
      " |              or in the legacy HDF5 format (`.h5`).\n",
      " |              Defaults to `\"tf\"` in TF 2.X, and `\"h5\"` in TF 1.X.\n",
      " |      \n",
      " |      SavedModel format arguments:\n",
      " |          include_optimizer: Only applied to SavedModel and legacy HDF5\n",
      " |              formats. If False, do not save the optimizer state.\n",
      " |              Defaults to `True`.\n",
      " |          signatures: Only applies to SavedModel format. Signatures to save\n",
      " |              with the SavedModel. See the `signatures` argument in\n",
      " |              `tf.saved_model.save` for details.\n",
      " |          options: Only applies to SavedModel format.\n",
      " |              `tf.saved_model.SaveOptions` object that specifies SavedModel\n",
      " |              saving options.\n",
      " |          save_traces: Only applies to SavedModel format. When enabled, the\n",
      " |              SavedModel will store the function traces for each layer. This\n",
      " |              can be disabled, so that only the configs of each layer are\n",
      " |              stored. Defaults to `True`.\n",
      " |              Disabling this will decrease serialization time\n",
      " |              and reduce file size, but it requires that all custom\n",
      " |              layers/models implement a `get_config()` method.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = tf.keras.Sequential([\n",
      " |          tf.keras.layers.Dense(5, input_shape=(3,)),\n",
      " |          tf.keras.layers.Softmax()])\n",
      " |      model.save(\"model.keras\")\n",
      " |      loaded_model = tf.keras.models.load_model(\"model.keras\")\n",
      " |      x = tf.random.uniform((10, 3))\n",
      " |      assert np.allclose(model.predict(x), loaded_model.predict(x))\n",
      " |      ```\n",
      " |      \n",
      " |      Note that `model.save()` is an alias for `tf.keras.models.save_model()`.\n",
      " |  \n",
      " |  save_spec(self, dynamic_batch=True)\n",
      " |      Returns the `tf.TensorSpec` of call args as a tuple `(args, kwargs)`.\n",
      " |      \n",
      " |      This value is automatically defined after calling the model for the\n",
      " |      first time. Afterwards, you can use it when exporting the model for\n",
      " |      serving:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = tf.keras.Model(...)\n",
      " |      \n",
      " |      @tf.function\n",
      " |      def serve(*args, **kwargs):\n",
      " |        outputs = model(*args, **kwargs)\n",
      " |        # Apply postprocessing steps, or add additional outputs.\n",
      " |        ...\n",
      " |        return outputs\n",
      " |      \n",
      " |      # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this\n",
      " |      # example, is an empty dict since functional models do not use keyword\n",
      " |      # arguments.\n",
      " |      arg_specs, kwarg_specs = model.save_spec()\n",
      " |      \n",
      " |      model.save(path, signatures={\n",
      " |        'serving_default': serve.get_concrete_function(*arg_specs,\n",
      " |                                                       **kwarg_specs)\n",
      " |      })\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        dynamic_batch: Whether to set the batch sizes of all the returned\n",
      " |          `tf.TensorSpec` to `None`. (Note that when defining functional or\n",
      " |          Sequential models with `tf.keras.Input([...], batch_size=X)`, the\n",
      " |          batch size will always be preserved). Defaults to `True`.\n",
      " |      Returns:\n",
      " |        If the model inputs are defined, returns a tuple `(args, kwargs)`. All\n",
      " |        elements in `args` and `kwargs` are `tf.TensorSpec`.\n",
      " |        If the model inputs are not defined, returns `None`.\n",
      " |        The model inputs are automatically set when calling the model,\n",
      " |        `model.fit`, `model.evaluate` or `model.predict`.\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
      " |      Saves all layer weights.\n",
      " |      \n",
      " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
      " |      argument.\n",
      " |      \n",
      " |      When saving in HDF5 format, the weight file has:\n",
      " |        - `layer_names` (attribute), a list of strings\n",
      " |            (ordered names of model layers).\n",
      " |        - For every layer, a `group` named `layer.name`\n",
      " |            - For every such layer group, a group attribute `weight_names`,\n",
      " |                a list of strings\n",
      " |                (ordered names of weights tensor of the layer).\n",
      " |            - For every weight in the layer, a dataset\n",
      " |                storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      When saving in TensorFlow format, all objects referenced by the network\n",
      " |      are saved in the same format as `tf.train.Checkpoint`, including any\n",
      " |      `Layer` instances or `Optimizer` instances assigned to object\n",
      " |      attributes. For networks constructed from inputs and outputs using\n",
      " |      `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network\n",
      " |      are tracked/saved automatically. For user-defined classes which inherit\n",
      " |      from `tf.keras.Model`, `Layer` instances must be assigned to object\n",
      " |      attributes, typically in the constructor. See the documentation of\n",
      " |      `tf.train.Checkpoint` and `tf.keras.Model` for details.\n",
      " |      \n",
      " |      While the formats are the same, do not mix `save_weights` and\n",
      " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should\n",
      " |      be loaded using `Model.load_weights`. Checkpoints saved using\n",
      " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
      " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
      " |      `save_weights` for training checkpoints.\n",
      " |      \n",
      " |      The TensorFlow format matches objects and variables by starting at a\n",
      " |      root object, `self` for `save_weights`, and greedily matching attribute\n",
      " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save`\n",
      " |      this is the `Checkpoint` even if the `Checkpoint` has a model attached.\n",
      " |      This means saving a `tf.keras.Model` using `save_weights` and loading\n",
      " |      into a `tf.train.Checkpoint` with a `Model` attached (or vice versa)\n",
      " |      will not match the `Model`'s variables. See the\n",
      " |      [guide to training checkpoints](\n",
      " |      https://www.tensorflow.org/guide/checkpoint) for details on\n",
      " |      the TensorFlow format.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: String or PathLike, path to the file to save the weights\n",
      " |              to. When saving in TensorFlow format, this is the prefix used\n",
      " |              for checkpoint files (multiple files are generated). Note that\n",
      " |              the '.h5' suffix causes weights to be saved in HDF5 format.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
      " |              '.keras' will default to HDF5 if `save_format` is `None`.\n",
      " |              Otherwise, `None` becomes 'tf'. Defaults to `None`.\n",
      " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
      " |              options for saving weights.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: If `h5py` is not available when attempting to save in\n",
      " |              HDF5 format.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None, expand_nested=False, show_trainable=False, layer_range=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      Args:\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided, becomes\n",
      " |              `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.\n",
      " |          print_fn: Print function to use. By default, prints to `stdout`.\n",
      " |              If `stdout` doesn't work in your environment, change to `print`.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |          expand_nested: Whether to expand the nested models.\n",
      " |              Defaults to `False`.\n",
      " |          show_trainable: Whether to show if a layer is trainable.\n",
      " |              Defaults to `False`.\n",
      " |          layer_range: a list or tuple of 2 strings,\n",
      " |              which is the starting layer name and ending layer name\n",
      " |              (both inclusive) indicating the range of layers to be printed\n",
      " |              in summary. It also accepts regex patterns instead of exact\n",
      " |              name. In such case, start predicate will be the first element\n",
      " |              it matches to `layer_range[0]` and the end predicate will be\n",
      " |              the last element it matches to `layer_range[1]`.\n",
      " |              By default `None` which considers all layers of model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if `summary()` is called before the model is built.\n",
      " |  \n",
      " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays (in case the\n",
      " |                model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors (in case the model has\n",
      " |                multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |                if the model has named inputs.\n",
      " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
      " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
      " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |            weights to apply to the model's loss for each sample. In the case\n",
      " |            of temporal data, you can pass a 2D array with shape (samples,\n",
      " |            sequence_length), to apply a different weight to every timestep of\n",
      " |            every sample.\n",
      " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
      " |            batch. If `False`, the metrics will be statefully accumulated\n",
      " |            across batches.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |            dict, with each key being the name of the metric. If `False`, they\n",
      " |            are returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If `model.test_on_batch` is wrapped in a\n",
      " |            `tf.function`.\n",
      " |  \n",
      " |  test_step(self, data)\n",
      " |      The logic for one evaluation step.\n",
      " |      \n",
      " |      This method can be overridden to support custom evaluation logic.\n",
      " |      This method is called by `Model.make_test_function`.\n",
      " |      \n",
      " |      This function should contain the mathematical logic for one step of\n",
      " |      evaluation.\n",
      " |      This typically includes the forward pass, loss calculation, and metrics\n",
      " |      updates.\n",
      " |      \n",
      " |      Configuration details for *how* this logic is run (e.g. `tf.function`\n",
      " |      and `tf.distribute.Strategy` settings), should be left to\n",
      " |      `Model.make_test_function`, which can also be overridden.\n",
      " |      \n",
      " |      Args:\n",
      " |        data: A nested structure of `Tensor`s.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `dict` containing values that will be passed to\n",
      " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
      " |        values of the `Model`'s metrics are returned.\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Additional keyword arguments to be passed to\n",
      " |              *`json.dumps()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      Note: Since TF 2.6, this method is no longer supported and will raise a\n",
      " |      RuntimeError.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A YAML string.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: announces that the method poses a security risk\n",
      " |  \n",
      " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |                (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |                (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |                if the model has named inputs.\n",
      " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
      " |            array(s) or TensorFlow tensor(s).\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |            weights to apply to the model's loss for each sample. In the case\n",
      " |            of temporal data, you can pass a 2D array with shape (samples,\n",
      " |            sequence_length), to apply a different weight to every timestep of\n",
      " |            every sample.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |            to a weight (float) to apply to the model's loss for the samples\n",
      " |            from this class during training. This can be useful to tell the\n",
      " |            model to \"pay more attention\" to samples from an under-represented\n",
      " |            class. When `class_weight` is specified and targets have a rank of\n",
      " |            2 or greater, either `y` must be one-hot encoded, or an explicit\n",
      " |            final dimension of `1` must be included for sparse class labels.\n",
      " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
      " |            batch. If `False`, the metrics will be statefully accumulated\n",
      " |            across batches.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |            dict, with each key being the name of the metric. If `False`, they\n",
      " |            are returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar training loss\n",
      " |          (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`.\n",
      " |  \n",
      " |  train_step(self, data)\n",
      " |      The logic for one training step.\n",
      " |      \n",
      " |      This method can be overridden to support custom training logic.\n",
      " |      For concrete examples of how to override this method see\n",
      " |      [Customizing what happens in fit](\n",
      " |      https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n",
      " |      This method is called by `Model.make_train_function`.\n",
      " |      \n",
      " |      This method should contain the mathematical logic for one step of\n",
      " |      training.  This typically includes the forward pass, loss calculation,\n",
      " |      backpropagation, and metric updates.\n",
      " |      \n",
      " |      Configuration details for *how* this logic is run (e.g. `tf.function`\n",
      " |      and `tf.distribute.Strategy` settings), should be left to\n",
      " |      `Model.make_train_function`, which can also be overridden.\n",
      " |      \n",
      " |      Args:\n",
      " |        data: A nested structure of `Tensor`s.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `dict` containing values that will be passed to\n",
      " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
      " |        values of the `Model`'s metrics are returned. Example:\n",
      " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.src.engine.training.Model:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.engine.training.Model:\n",
      " |  \n",
      " |  distribute_strategy\n",
      " |      The `tf.distribute.Strategy` this model was created under.\n",
      " |  \n",
      " |  metrics\n",
      " |      Return metrics added using `compile()` or `add_metric()`.\n",
      " |      \n",
      " |      Note: Metrics passed to `compile()` are available only after a\n",
      " |      `keras.Model` has been trained/evaluated on actual data.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
      " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
      " |      >>> [m.name for m in model.metrics]\n",
      " |      []\n",
      " |      \n",
      " |      >>> x = np.random.random((2, 3))\n",
      " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
      " |      >>> model.fit(x, y)\n",
      " |      >>> [m.name for m in model.metrics]\n",
      " |      ['loss', 'mae']\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
      " |      >>> output_1 = d(inputs)\n",
      " |      >>> output_2 = d(inputs)\n",
      " |      >>> model = tf.keras.models.Model(\n",
      " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
      " |      >>> model.add_metric(\n",
      " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
      " |      >>> model.fit(x, (y, y))\n",
      " |      >>> [m.name for m in model.metrics]\n",
      " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
      " |      'out_1_acc', 'mean']\n",
      " |  \n",
      " |  metrics_names\n",
      " |      Returns the model's display labels for all outputs.\n",
      " |      \n",
      " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
      " |      trained/evaluated on actual data.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
      " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
      " |      >>> model.metrics_names\n",
      " |      []\n",
      " |      \n",
      " |      >>> x = np.random.random((2, 3))\n",
      " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
      " |      >>> model.fit(x, y)\n",
      " |      >>> model.metrics_names\n",
      " |      ['loss', 'mae']\n",
      " |      \n",
      " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
      " |      >>> output_1 = d(inputs)\n",
      " |      >>> output_2 = d(inputs)\n",
      " |      >>> model = tf.keras.models.Model(\n",
      " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
      " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
      " |      >>> model.fit(x, (y, y))\n",
      " |      >>> model.metrics_names\n",
      " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
      " |      'out_1_acc']\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are\n",
      " |      expected to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  state_updates\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are\n",
      " |      not themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.engine.training.Model:\n",
      " |  \n",
      " |  distribute_reduction_method\n",
      " |      The method employed to reduce per-replica values during training.\n",
      " |      \n",
      " |      Unless specified, the value \"auto\" will be assumed, indicating that\n",
      " |      the reduction strategy should be chosen based on the current\n",
      " |      running environment.\n",
      " |      See `reduce_per_replica` function for more details.\n",
      " |  \n",
      " |  jit_compile\n",
      " |      Specify whether to compile the model with XLA.\n",
      " |      \n",
      " |      [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n",
      " |      for machine learning. `jit_compile` is not enabled by default.\n",
      " |      Note that `jit_compile=True` may not necessarily work for all models.\n",
      " |      \n",
      " |      For more information on supported operations please refer to the\n",
      " |      [XLA documentation](https://www.tensorflow.org/xla). Also refer to\n",
      " |      [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n",
      " |      for more details.\n",
      " |  \n",
      " |  run_eagerly\n",
      " |      Settable attribute indicating whether the model should run eagerly.\n",
      " |      \n",
      " |      Running eagerly means that your model will be run step by step,\n",
      " |      like Python code. Your model might run slower, but it should become\n",
      " |      easier for you to debug it by stepping into individual layer calls.\n",
      " |      \n",
      " |      By default, we will attempt to compile your model to a static graph to\n",
      " |      deliver the best execution performance.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Boolean, whether the model should run eagerly.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be\n",
      " |      dependent on the inputs passed when calling a layer. Hence, when reusing\n",
      " |      the same layer on different inputs `a` and `b`, some entries in\n",
      " |      `layer.losses` may be dependent on `a` and some on `b`. This method\n",
      " |      automatically keeps track of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      The same code works in distributed training: the input to `add_loss()`\n",
      " |      is treated like a regularization loss and averaged across replicas\n",
      " |      by the training loop (both built-in `Model.fit()` and compliant custom\n",
      " |      training loops).\n",
      " |      \n",
      " |      The `add_loss` method can also be called directly on a Functional Model\n",
      " |      during construction. In this case, any loss Tensors passed to this Model\n",
      " |      must be symbolic and be able to be traced back to the model's `Input`s.\n",
      " |      These losses become part of the model's topology and are tracked in\n",
      " |      `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss\n",
      " |      references a `Variable` of one of the model's layers), you can wrap your\n",
      " |      loss in a zero-argument lambda. These losses are not tracked as part of\n",
      " |      the model's topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n",
      " |          losses may also be zero-argument callables which create a loss\n",
      " |          tensor.\n",
      " |        **kwargs: Used for backwards compatibility only.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This\n",
      " |      is because we cannot trace the metric result tensor back to the model's\n",
      " |      inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result\n",
      " |          of calling a `keras.Metric` instance, it will be aggregated by\n",
      " |          default using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and\n",
      " |      variance in a BatchNormalization layer) may be dependent on the inputs\n",
      " |      passed when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case,\n",
      " |      variable updates are run on the fly and thus do not need to be tracked\n",
      " |      for later execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use a `ResourceVariable` or not.\n",
      " |          See [this guide](\n",
      " |          https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n",
      " |           for more information.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set\n",
      " |          to `AUTO` and the current `DistributionStrategy` chooses when to\n",
      " |          synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n",
      " |          must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as\n",
      " |          `ON_READ`.\n",
      " |  \n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |      \n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects,\n",
      " |          describing how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after\n",
      " |      updating a layer weights. It can be overridden to finalize any\n",
      " |      additional layer state after a weight update.\n",
      " |      \n",
      " |      This function will be called after weights of a layer have been restored\n",
      " |      from a loaded model.\n",
      " |  \n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |      \n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |      \n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
      " |      matrix and the bias vector. These can be used to set the weights of\n",
      " |      another `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which\n",
      " |      causes computations and the output to be in the compute dtype as well.\n",
      " |      This is done by the base Layer class in `Layer.__call__`, so you do not\n",
      " |      have to insert these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision\n",
      " |      when `compute_dtype` is float16 or bfloat16 for numeric stability. The\n",
      " |      output will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Return Functional API nodes upstream of this layer.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is\n",
      " |      accessed, so it is eager safe: accessing `losses` under a\n",
      " |      `tf.GradientTape` will propagate gradients back to the corresponding\n",
      " |      variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Return Functional API nodes downstream of this layer.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are\n",
      " |      not themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.models.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "927b343e-3581-40cb-9314-1ce43cca2adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MeanSquaredError in module keras.src.losses:\n",
      "\n",
      "class MeanSquaredError(LossFunctionWrapper)\n",
      " |  MeanSquaredError(reduction='auto', name='mean_squared_error')\n",
      " |  \n",
      " |  Computes the mean of squares of errors between labels and predictions.\n",
      " |  \n",
      " |  `loss = mean(square(y_true - y_pred))`\n",
      " |  \n",
      " |  Standalone usage:\n",
      " |  \n",
      " |  >>> y_true = [[0., 1.], [0., 0.]]\n",
      " |  >>> y_pred = [[1., 1.], [1., 0.]]\n",
      " |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      " |  >>> mse = tf.keras.losses.MeanSquaredError()\n",
      " |  >>> mse(y_true, y_pred).numpy()\n",
      " |  0.5\n",
      " |  \n",
      " |  >>> # Calling with 'sample_weight'.\n",
      " |  >>> mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()\n",
      " |  0.25\n",
      " |  \n",
      " |  >>> # Using 'sum' reduction type.\n",
      " |  >>> mse = tf.keras.losses.MeanSquaredError(\n",
      " |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      " |  >>> mse(y_true, y_pred).numpy()\n",
      " |  1.0\n",
      " |  \n",
      " |  >>> # Using 'none' reduction type.\n",
      " |  >>> mse = tf.keras.losses.MeanSquaredError(\n",
      " |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      " |  >>> mse(y_true, y_pred).numpy()\n",
      " |  array([0.5, 0.5], dtype=float32)\n",
      " |  \n",
      " |  Usage with the `compile()` API:\n",
      " |  \n",
      " |  ```python\n",
      " |  model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError())\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MeanSquaredError\n",
      " |      LossFunctionWrapper\n",
      " |      Loss\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, reduction='auto', name='mean_squared_error')\n",
      " |      Initializes `MeanSquaredError` instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      " |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      " |          option will be determined by the usage context. For almost all cases\n",
      " |          this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n",
      " |          `tf.distribute.Strategy`, except via `Model.compile()` and\n",
      " |          `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n",
      " |          will raise an error. Please see this custom training [tutorial](\n",
      " |          https://www.tensorflow.org/tutorials/distribute/custom_training)\n",
      " |          for more details.\n",
      " |        name: Optional name for the instance. Defaults to\n",
      " |          'mean_squared_error'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LossFunctionWrapper:\n",
      " |  \n",
      " |  call(self, y_true, y_pred)\n",
      " |      Invokes the `LossFunctionWrapper` instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        y_true: Ground truth values.\n",
      " |        y_pred: The predicted values.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Loss values per sample.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config dictionary for a `Loss` instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from LossFunctionWrapper:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Output of `get_config()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `keras.losses.Loss` instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Loss:\n",
      " |  \n",
      " |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      " |      Invokes the `Loss` instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      " |          sparse loss functions such as sparse categorical crossentropy where\n",
      " |          shape = `[batch_size, d0, .. dN-1]`\n",
      " |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      " |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      " |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      " |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      " |          then the total loss for each sample of the batch is rescaled by the\n",
      " |          corresponding element in the `sample_weight` vector. If the shape of\n",
      " |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      " |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      " |          scaled by the corresponding value of `sample_weight`. (Note\n",
      " |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      " |          axis=-1.)\n",
      " |      \n",
      " |      Returns:\n",
      " |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      " |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      " |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      " |          axis=-1.)\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the shape of `sample_weight` is invalid.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Loss:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.losses.MeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e3dc5a62-67d6-412f-b15c-ec36a098af59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.metrics` module includes score functions, performance metrics\n",
      "    and pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _classification\n",
      "    _dist_metrics\n",
      "    _pairwise_distances_reduction (package)\n",
      "    _pairwise_fast\n",
      "    _plot (package)\n",
      "    _ranking\n",
      "    _regression\n",
      "    _scorer\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.metrics._dist_metrics.DistanceMetric\n",
      "        sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay\n",
      "        sklearn.metrics._plot.det_curve.DetCurveDisplay\n",
      "        sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay\n",
      "        sklearn.metrics._plot.regression.PredictionErrorDisplay\n",
      "        sklearn.metrics._plot.roc_curve.RocCurveDisplay\n",
      "    \n",
      "    class ConfusionMatrixDisplay(builtins.object)\n",
      "     |  ConfusionMatrixDisplay(confusion_matrix, *, display_labels=None)\n",
      "     |  \n",
      "     |  Confusion Matrix visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_predictions` to\n",
      "     |  create a :class:`ConfusionMatrixDisplay`. All parameters are stored as\n",
      "     |  attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  confusion_matrix : ndarray of shape (n_classes, n_classes)\n",
      "     |      Confusion matrix.\n",
      "     |  \n",
      "     |  display_labels : ndarray of shape (n_classes,), default=None\n",
      "     |      Display labels for plot. If None, display labels are set from 0 to\n",
      "     |      `n_classes - 1`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  im_ : matplotlib AxesImage\n",
      "     |      Image representing the confusion matrix.\n",
      "     |  \n",
      "     |  text_ : ndarray of shape (n_classes, n_classes), dtype=matplotlib Text,             or None\n",
      "     |      Array of matplotlib axes. `None` if `include_values` is false.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with confusion matrix.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the confusion matrix.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "     |      classification.\n",
      "     |  ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |      given an estimator, the data, and the label.\n",
      "     |  ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |      given the true and predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
      "     |  >>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
      "     |  ...                               display_labels=clf.classes_)\n",
      "     |  >>> disp.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, confusion_matrix, *, display_labels=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, *, include_values=True, cmap='viridis', xticks_rotation='horizontal', values_format=None, ax=None, colorbar=True, im_kw=None, text_kw=None)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                          default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`,\n",
      "     |          the format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |          Returns a :class:`~sklearn.metrics.ConfusionMatrixDisplay` instance\n",
      "     |          that contains all the information to plot the confusion matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True, im_kw=None, text_kw=None) from builtins.type\n",
      "     |      Plot Confusion Matrix given an estimator and some data.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      labels : array-like of shape (n_classes,), default=None\n",
      "     |          List of labels to index the confusion matrix. This may be used to\n",
      "     |          reorder or select a subset of labels. If `None` is given, those\n",
      "     |          that appear at least once in `y_true` or `y_pred` are used in\n",
      "     |          sorted order.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      normalize : {'true', 'pred', 'all'}, default=None\n",
      "     |          Either to normalize the counts display in the matrix:\n",
      "     |      \n",
      "     |          - if `'true'`, the confusion matrix is normalized over the true\n",
      "     |            conditions (e.g. rows);\n",
      "     |          - if `'pred'`, the confusion matrix is normalized over the\n",
      "     |            predicted conditions (e.g. columns);\n",
      "     |          - if `'all'`, the confusion matrix is normalized by the total\n",
      "     |            number of samples;\n",
      "     |          - if `None` (default), the confusion matrix will not be normalized.\n",
      "     |      \n",
      "     |      display_labels : array-like of shape (n_classes,), default=None\n",
      "     |          Target names used for plotting. By default, `labels` will be used\n",
      "     |          if it is defined, otherwise the unique labels of `y_true` and\n",
      "     |          `y_pred` will be used.\n",
      "     |      \n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                 default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`, the\n",
      "     |          format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |          given the true and predicted labels.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import ConfusionMatrixDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      SVC(random_state=0)\n",
      "     |      >>> ConfusionMatrixDisplay.from_estimator(\n",
      "     |      ...     clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True, im_kw=None, text_kw=None) from builtins.type\n",
      "     |      Plot Confusion Matrix given true and predicted labels.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          The predicted labels given by the method `predict` of an\n",
      "     |          classifier.\n",
      "     |      \n",
      "     |      labels : array-like of shape (n_classes,), default=None\n",
      "     |          List of labels to index the confusion matrix. This may be used to\n",
      "     |          reorder or select a subset of labels. If `None` is given, those\n",
      "     |          that appear at least once in `y_true` or `y_pred` are used in\n",
      "     |          sorted order.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      normalize : {'true', 'pred', 'all'}, default=None\n",
      "     |          Either to normalize the counts display in the matrix:\n",
      "     |      \n",
      "     |          - if `'true'`, the confusion matrix is normalized over the true\n",
      "     |            conditions (e.g. rows);\n",
      "     |          - if `'pred'`, the confusion matrix is normalized over the\n",
      "     |            predicted conditions (e.g. columns);\n",
      "     |          - if `'all'`, the confusion matrix is normalized by the total\n",
      "     |            number of samples;\n",
      "     |          - if `None` (default), the confusion matrix will not be normalized.\n",
      "     |      \n",
      "     |      display_labels : array-like of shape (n_classes,), default=None\n",
      "     |          Target names used for plotting. By default, `labels` will be used\n",
      "     |          if it is defined, otherwise the unique labels of `y_true` and\n",
      "     |          `y_pred` will be used.\n",
      "     |      \n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                 default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`, the\n",
      "     |          format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |          given an estimator, the data, and the label.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import ConfusionMatrixDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      SVC(random_state=0)\n",
      "     |      >>> y_pred = clf.predict(X_test)\n",
      "     |      >>> ConfusionMatrixDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DetCurveDisplay(builtins.object)\n",
      "     |  DetCurveDisplay(*, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  DET curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.DetCurveDisplay.from_estimator`\n",
      "     |  or :func:`~sklearn.metrics.DetCurveDisplay.from_predictions` to create a\n",
      "     |  visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  fnr : ndarray\n",
      "     |      False negative rate.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The label of the positive class.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      DET Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with DET Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  det_curve : Compute error rates for different probability thresholds.\n",
      "     |  DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "     |      some data.\n",
      "     |  DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "     |      predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import det_curve, DetCurveDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, test_size=0.4, random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |  >>> y_pred = clf.decision_function(X_test)\n",
      "     |  >>> fpr, fnr, _ = det_curve(y_test, y_pred)\n",
      "     |  >>> display = DetCurveDisplay(\n",
      "     |  ...     fpr=fpr, fnr=fnr, estimator_name=\"SVC\"\n",
      "     |  ... )\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use `estimator_name` if\n",
      "     |          it is not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, response_method='auto', pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot DET curve given an estimator and data.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the predicted target response. If set\n",
      "     |          to 'auto', :term:`predict_proba` is tried first and if it does not\n",
      "     |          exist :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      det_curve : Compute error rates for different probability thresholds.\n",
      "     |      DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "     |          predicted labels.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import DetCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, test_size=0.4, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> DetCurveDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot the DET curve given the true and predicted labels.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by `decision_function` on some classifiers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      det_curve : Compute error rates for different probability thresholds.\n",
      "     |      DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "     |          some data.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import DetCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, test_size=0.4, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> y_pred = clf.decision_function(X_test)\n",
      "     |      >>> DetCurveDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DistanceMetric(builtins.object)\n",
      "     |  DistanceMetric class\n",
      "     |  \n",
      "     |  This class provides a uniform interface to fast distance metric\n",
      "     |  functions.  The various metrics can be accessed via the :meth:`get_metric`\n",
      "     |  class method and the metric string identifier (see below).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.metrics import DistanceMetric\n",
      "     |  >>> dist = DistanceMetric.get_metric('euclidean')\n",
      "     |  >>> X = [[0, 1, 2],\n",
      "     |           [3, 4, 5]]\n",
      "     |  >>> dist.pairwise(X)\n",
      "     |  array([[ 0.        ,  5.19615242],\n",
      "     |         [ 5.19615242,  0.        ]])\n",
      "     |  \n",
      "     |  Available Metrics\n",
      "     |  \n",
      "     |  The following lists the string metric identifiers and the associated\n",
      "     |  distance metric classes:\n",
      "     |  \n",
      "     |  **Metrics intended for real-valued vector spaces:**\n",
      "     |  \n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  identifier      class name            args      distance function\n",
      "     |  --------------  --------------------  --------  -------------------------------\n",
      "     |  \"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n",
      "     |  \"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n",
      "     |  \"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n",
      "     |  \"minkowski\"     MinkowskiDistance     p, w      ``sum(w * |x - y|^p)^(1/p)``\n",
      "     |  \"wminkowski\"    WMinkowskiDistance    p, w      ``sum(|w * (x - y)|^p)^(1/p)``\n",
      "     |  \"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n",
      "     |  \"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)' V^-1 (x - y))``\n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  \n",
      "     |  .. deprecated:: 1.1\n",
      "     |      `WMinkowskiDistance` is deprecated in version 1.1 and will be removed in version 1.3.\n",
      "     |      Use `MinkowskiDistance` instead. Note that in `MinkowskiDistance`, the weights are\n",
      "     |      applied to the absolute differences already raised to the p power. This is different from\n",
      "     |      `WMinkowskiDistance` where weights are applied to the absolute differences before raising\n",
      "     |      to the p power. The deprecation aims to remain consistent with SciPy 1.8 convention.\n",
      "     |  \n",
      "     |  **Metrics intended for two-dimensional vector spaces:**  Note that the haversine\n",
      "     |  distance metric requires data in the form of [latitude, longitude] and both\n",
      "     |  inputs and outputs are in units of radians.\n",
      "     |  \n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  identifier    class name          distance function\n",
      "     |  ------------  ------------------  ---------------------------------------------------------------\n",
      "     |  \"haversine\"   HaversineDistance   ``2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy)))``\n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  \n",
      "     |  \n",
      "     |  **Metrics intended for integer-valued vector spaces:**  Though intended\n",
      "     |  for integer-valued vectors, these are also valid metrics in the case of\n",
      "     |  real-valued vectors.\n",
      "     |  \n",
      "     |  =============  ====================  ========================================\n",
      "     |  identifier     class name            distance function\n",
      "     |  -------------  --------------------  ----------------------------------------\n",
      "     |  \"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n",
      "     |  \"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n",
      "     |  \"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n",
      "     |  =============  ====================  ========================================\n",
      "     |  \n",
      "     |  **Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\n",
      "     |  is evaluated to \"True\".  In the listings below, the following\n",
      "     |  abbreviations are used:\n",
      "     |  \n",
      "     |   - N  : number of dimensions\n",
      "     |   - NTT : number of dims in which both values are True\n",
      "     |   - NTF : number of dims in which the first value is True, second is False\n",
      "     |   - NFT : number of dims in which the first value is False, second is True\n",
      "     |   - NFF : number of dims in which both values are False\n",
      "     |   - NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
      "     |   - NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
      "     |  \n",
      "     |  =================  =======================  ===============================\n",
      "     |  identifier         class name               distance function\n",
      "     |  -----------------  -----------------------  -------------------------------\n",
      "     |  \"jaccard\"          JaccardDistance          NNEQ / NNZ\n",
      "     |  \"matching\"         MatchingDistance         NNEQ / N\n",
      "     |  \"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n",
      "     |  \"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n",
      "     |  \"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n",
      "     |  \"russellrao\"       RussellRaoDistance       (N - NTT) / N\n",
      "     |  \"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n",
      "     |  \"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n",
      "     |  =================  =======================  ===============================\n",
      "     |  \n",
      "     |  **User-defined distance:**\n",
      "     |  \n",
      "     |  ===========    ===============    =======\n",
      "     |  identifier     class name         args\n",
      "     |  -----------    ---------------    -------\n",
      "     |  \"pyfunc\"       PyFuncDistance     func\n",
      "     |  ===========    ===============    =======\n",
      "     |  \n",
      "     |  Here ``func`` is a function which takes two one-dimensional numpy\n",
      "     |  arrays, and returns a distance.  Note that in order to be used within\n",
      "     |  the BallTree, the distance must be a true metric:\n",
      "     |  i.e. it must satisfy the following properties\n",
      "     |  \n",
      "     |  1) Non-negativity: d(x, y) >= 0\n",
      "     |  2) Identity: d(x, y) = 0 if and only if x == y\n",
      "     |  3) Symmetry: d(x, y) = d(y, x)\n",
      "     |  4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
      "     |  \n",
      "     |  Because of the Python object overhead involved in calling the python\n",
      "     |  function, this will be fairly slow, but it will have the same\n",
      "     |  scaling as other distances.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |      get state for pickling\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      reduce method used for pickling\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |      set state for pickling\n",
      "     |  \n",
      "     |  dist_to_rdist(...)\n",
      "     |      Convert the true distance to the rank-preserving surrogate distance.\n",
      "     |      \n",
      "     |      The surrogate distance is any measure that yields the same rank as the\n",
      "     |      distance, but is more efficient to compute. For example, the\n",
      "     |      rank-preserving surrogate distance of the Euclidean metric is the\n",
      "     |      squared-euclidean distance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dist : double\n",
      "     |          True distance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          Surrogate distance.\n",
      "     |  \n",
      "     |  pairwise(...)\n",
      "     |      Compute the pairwise distances between X and Y\n",
      "     |      \n",
      "     |      This is a convenience routine for the sake of testing.  For many\n",
      "     |      metrics, the utilities in scipy.spatial.distance.cdist and\n",
      "     |      scipy.spatial.distance.pdist will be faster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray or CSR matrix of shape (n_samples_X, n_features)\n",
      "     |          Input data.\n",
      "     |      Y : ndarray or CSR matrix of shape (n_samples_Y, n_features)\n",
      "     |          Input data.\n",
      "     |          If not specified, then Y=X.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dist : ndarray of shape  (n_samples_X, n_samples_Y)\n",
      "     |          The distance matrix of pairwise distances between points in X and Y.\n",
      "     |  \n",
      "     |  rdist_to_dist(...)\n",
      "     |      Convert the rank-preserving surrogate distance to the distance.\n",
      "     |      \n",
      "     |      The surrogate distance is any measure that yields the same rank as the\n",
      "     |      distance, but is more efficient to compute. For example, the\n",
      "     |      rank-preserving surrogate distance of the Euclidean metric is the\n",
      "     |      squared-euclidean distance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rdist : double\n",
      "     |          Surrogate distance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          True distance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  get_metric(...) from builtins.type\n",
      "     |      Get the given distance metric from the string identifier.\n",
      "     |      \n",
      "     |      See the docstring of DistanceMetric for a list of available metrics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      metric : str or class name\n",
      "     |          The distance metric to use\n",
      "     |      **kwargs\n",
      "     |          additional arguments will be passed to the requested metric\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "    \n",
      "    class PrecisionRecallDisplay(builtins.object)\n",
      "     |  PrecisionRecallDisplay(precision, recall, *, average_precision=None, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  Precision Recall visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.PrecisionRecallDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.PrecisionRecallDisplay.from_predictions` to create\n",
      "     |  a :class:`~sklearn.metrics.PredictionRecallDisplay`. All parameters are\n",
      "     |  stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  precision : ndarray\n",
      "     |      Precision values.\n",
      "     |  \n",
      "     |  recall : ndarray\n",
      "     |      Recall values.\n",
      "     |  \n",
      "     |  average_precision : float, default=None\n",
      "     |      Average precision. If None, the average precision is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, then the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The class considered as the positive class. If None, the class will not\n",
      "     |      be shown in the legend.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Precision recall curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with precision recall curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  precision_recall_curve : Compute precision-recall pairs for different\n",
      "     |      probability thresholds.\n",
      "     |  PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n",
      "     |      a binary classifier.\n",
      "     |  PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n",
      "     |      using predictions from a binary classifier.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The average precision (cf. :func:`~sklearn.metrics.average_precision`) in\n",
      "     |  scikit-learn is computed without any interpolation. To be consistent with\n",
      "     |  this metric, the precision-recall curve is plotted without any\n",
      "     |  interpolation as well (step-wise style).\n",
      "     |  \n",
      "     |  You can change this style by passing the keyword argument\n",
      "     |  `drawstyle=\"default\"` in :meth:`plot`, :meth:`from_estimator`, or\n",
      "     |  :meth:`from_predictions`. However, the curve will not be strictly\n",
      "     |  consistent with the reported average precision.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import (precision_recall_curve,\n",
      "     |  ...                              PrecisionRecallDisplay)\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
      "     |  >>> disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
      "     |  >>> disp.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, precision, recall, *, average_precision=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : Matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of precision recall curve for labeling. If `None`, use\n",
      "     |          `estimator_name` if not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, pos_label=None, response_method='auto', name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot precision-recall curve given an estimator and some data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The class considered as the positive class when computing the\n",
      "     |          precision and recall metrics. By default, `estimators.classes_[1]`\n",
      "     |          is considered as the positive class.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'},             default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name for labeling curve. If `None`, no name is used.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PrecisionRecallDisplay.from_predictions : Plot precision-recall curve\n",
      "     |          using estimated probabilities or output of decision function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import PrecisionRecallDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = LogisticRegression()\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      LogisticRegression()\n",
      "     |      >>> PrecisionRecallDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot precision-recall curve given binary class predictions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True binary labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Estimated probabilities or output of decision function.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The class considered as the positive class when computing the\n",
      "     |          precision and recall metrics.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name for labeling curve. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PrecisionRecallDisplay.from_estimator : Plot precision-recall curve\n",
      "     |          using an estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import PrecisionRecallDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = LogisticRegression()\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      LogisticRegression()\n",
      "     |      >>> y_pred = clf.predict_proba(X_test)[:, 1]\n",
      "     |      >>> PrecisionRecallDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PredictionErrorDisplay(builtins.object)\n",
      "     |  PredictionErrorDisplay(*, y_true, y_pred)\n",
      "     |  \n",
      "     |  Visualization of the prediction error of a regression model.\n",
      "     |  \n",
      "     |  This tool can display \"residuals vs predicted\" or \"actual vs predicted\"\n",
      "     |  using scatter plots to qualitatively assess the behavior of a regressor,\n",
      "     |  preferably on held-out data points.\n",
      "     |  \n",
      "     |  See the details in the docstrings of\n",
      "     |  :func:`~sklearn.metrics.PredictionErrorDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.PredictionErrorDisplay.from_predictions` to\n",
      "     |  create a visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  For general information regarding `scikit-learn` visualization tools, read\n",
      "     |  more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |  For details regarding interpreting these plots, refer to the\n",
      "     |  :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  y_true : ndarray of shape (n_samples,)\n",
      "     |      True values.\n",
      "     |  \n",
      "     |  y_pred : ndarray of shape (n_samples,)\n",
      "     |      Prediction values.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Optimal line representing `y_true == y_pred`. Therefore, it is a\n",
      "     |      diagonal line for `kind=\"predictions\"` and a horizontal line for\n",
      "     |      `kind=\"residuals\"`.\n",
      "     |  \n",
      "     |  errors_lines_ : matplotlib Artist or None\n",
      "     |      Residual lines. If `with_errors=False`, then it is set to `None`.\n",
      "     |  \n",
      "     |  scatter_ : matplotlib Artist\n",
      "     |      Scatter data points.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with the different matplotlib axis.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the scatter and lines.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  PredictionErrorDisplay.from_estimator : Prediction error visualization\n",
      "     |      given an estimator and some data.\n",
      "     |  PredictionErrorDisplay.from_predictions : Prediction error visualization\n",
      "     |      given the true and predicted targets.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> ridge = Ridge().fit(X, y)\n",
      "     |  >>> y_pred = ridge.predict(X)\n",
      "     |  >>> display = PredictionErrorDisplay(y_true=y, y_pred=y_pred)\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, y_true, y_pred)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, kind='residual_vs_predicted', scatter_kwargs=None, line_kwargs=None)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's ``plot``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.PredictionErrorDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, kind='residual_vs_predicted', subsample=1000, random_state=None, ax=None, scatter_kwargs=None, line_kwargs=None) from builtins.type\n",
      "     |      Plot the prediction error given a regressor and some data.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools,\n",
      "     |      read more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For details regarding interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted regressor or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a regressor.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      subsample : float, int or None, default=1_000\n",
      "     |          Sampling the samples to be shown on the scatter plot. If `float`,\n",
      "     |          it should be between 0 and 1 and represents the proportion of the\n",
      "     |          original dataset. If `int`, it represents the number of samples\n",
      "     |          display on the scatter plot. If `None`, no subsampling will be\n",
      "     |          applied. by default, a 1000 samples or less will be displayed.\n",
      "     |      \n",
      "     |      random_state : int or RandomState, default=None\n",
      "     |          Controls the randomness when `subsample` is not `None`.\n",
      "     |          See :term:`Glossary <random_state>` for details.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PredictionErrorDisplay`\n",
      "     |          Object that stores the computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PredictionErrorDisplay : Prediction error visualization for regression.\n",
      "     |      PredictionErrorDisplay.from_predictions : Prediction error visualization\n",
      "     |          given the true and predicted targets.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import load_diabetes\n",
      "     |      >>> from sklearn.linear_model import Ridge\n",
      "     |      >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |      >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |      >>> ridge = Ridge().fit(X, y)\n",
      "     |      >>> disp = PredictionErrorDisplay.from_estimator(ridge, X, y)\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, kind='residual_vs_predicted', subsample=1000, random_state=None, ax=None, scatter_kwargs=None, line_kwargs=None) from builtins.type\n",
      "     |      Plot the prediction error given the true and predicted targets.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools,\n",
      "     |      read more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For details regarding interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True target values.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Predicted target values.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      subsample : float, int or None, default=1_000\n",
      "     |          Sampling the samples to be shown on the scatter plot. If `float`,\n",
      "     |          it should be between 0 and 1 and represents the proportion of the\n",
      "     |          original dataset. If `int`, it represents the number of samples\n",
      "     |          display on the scatter plot. If `None`, no subsampling will be\n",
      "     |          applied. by default, a 1000 samples or less will be displayed.\n",
      "     |      \n",
      "     |      random_state : int or RandomState, default=None\n",
      "     |          Controls the randomness when `subsample` is not `None`.\n",
      "     |          See :term:`Glossary <random_state>` for details.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PredictionErrorDisplay`\n",
      "     |          Object that stores the computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PredictionErrorDisplay : Prediction error visualization for regression.\n",
      "     |      PredictionErrorDisplay.from_estimator : Prediction error visualization\n",
      "     |          given an estimator and some data.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import load_diabetes\n",
      "     |      >>> from sklearn.linear_model import Ridge\n",
      "     |      >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |      >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |      >>> ridge = Ridge().fit(X, y)\n",
      "     |      >>> y_pred = ridge.predict(X)\n",
      "     |      >>> disp = PredictionErrorDisplay.from_predictions(y_true=y, y_pred=y_pred)\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RocCurveDisplay(builtins.object)\n",
      "     |  RocCurveDisplay(*, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  ROC Curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_predictions` to create\n",
      "     |  a :class:`~sklearn.metrics.RocCurveDisplay`. All parameters are\n",
      "     |  stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  tpr : ndarray\n",
      "     |      True positive rate.\n",
      "     |  \n",
      "     |  roc_auc : float, default=None\n",
      "     |      Area under ROC curve. If None, the roc_auc score is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The class considered as the positive class when computing the roc auc\n",
      "     |      metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |      as the positive class.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      ROC Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with ROC Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |  RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "     |      (ROC) curve given an estimator and some data.\n",
      "     |  RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "     |      (ROC) curve given the true and predicted values.\n",
      "     |  roc_auc_score : Compute the area under the ROC curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import metrics\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "     |  >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
      "     |  >>> roc_auc = metrics.auc(fpr, tpr)\n",
      "     |  >>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
      "     |  ...                                   estimator_name='example estimator')\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's ``plot``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use `estimator_name` if\n",
      "     |          not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Create a ROC Curve display from an estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted ROC curve. This is useful in order to create lighter\n",
      "     |          ROC curves.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The class considered as the positive class when computing the roc auc\n",
      "     |          metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |          as the positive class.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.RocCurveDisplay`\n",
      "     |          The ROC Curve display.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |      RocCurveDisplay.from_predictions : ROC Curve visualization given the\n",
      "     |          probabilities of scores of a classifier.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> RocCurveDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, drop_intermediate=True, pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot ROC curve given the true and predicted values.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by “decision_function” on some classifiers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted ROC curve. This is useful in order to create lighter\n",
      "     |          ROC curves.\n",
      "     |      \n",
      "     |      pos_label : str or int, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC curve for labeling. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |      RocCurveDisplay.from_estimator : ROC Curve visualization given an\n",
      "     |          estimator and some data.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> y_pred = clf.decision_function(X_test)\n",
      "     |      >>> RocCurveDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        balanced_accuracy_score : Compute the balanced accuracy to deal with\n",
      "            imbalanced datasets.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "            two sets of samples.\n",
      "        zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "            function will return the percentage of imperfectly predicted subsets.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary classification, this function is equal to the `jaccard_score`\n",
      "        function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching :math:`U` (``label_true``)\n",
      "        with :math:`V` (``labels_pred``) will return the same score value. This can\n",
      "        be useful to measure the agreement of two independent label assignments\n",
      "        strategies on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets, called :math:`U` in\n",
      "            the above formula.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets, called :math:`V` in\n",
      "            the above formula.\n",
      "        \n",
      "        average_method : str, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'max' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float (upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative. The value is\n",
      "           in adjusted nats (based on the natural logarithm).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        mutual_info_score : Mutual Information (not adjusted for chance).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation). The adjusted Rand index is bounded below by -0.5 for\n",
      "        especially discordant clusterings.\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ARI : float\n",
      "           Similarity score between -0.5 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie\n",
      "          adjusted Rand index, Psychological Methods 2004\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        .. [Chacon] :doi:`Minimum adjusted Rand index for two clusterings of a given size,\n",
      "          2022, J. E. Chacón and A. I. Rastrojo <10.1007/s11634-022-00491-w>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.57...\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n",
      "          0.57...\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        ARI may take a negative value for especially discordant labelings that\n",
      "        are a worse choice than the expected value of random labels::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 1, 0, 1])\n",
      "          -0.5\n",
      "    \n",
      "    auc(x, y)\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n",
      "        way to summarize a precision-recall curve, see\n",
      "        :func:`average_precision_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray of shape (n,)\n",
      "            X coordinates. These must be either monotonic increasing or monotonic\n",
      "            decreasing.\n",
      "        y : ndarray of shape, (n,)\n",
      "            Y coordinates.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "            Area Under the Curve.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "    \n",
      "    average_precision_score(y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores.\n",
      "        \n",
      "        AP summarizes a precision-recall curve as the weighted mean of precisions\n",
      "        achieved at each threshold, with the increase in recall from the previous\n",
      "        threshold used as the weight:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n",
      "        \n",
      "        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth\n",
      "        threshold [1]_. This implementation is not interpolated and is different\n",
      "        from computing the area under the precision-recall curve with the\n",
      "        trapezoidal rule, which uses linear interpolation and can be too\n",
      "        optimistic.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by :term:`decision_function` on some classifiers).\n",
      "        \n",
      "        average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        pos_label : int or str, default=1\n",
      "            The label of the positive class. Only applied to binary ``y_true``.\n",
      "            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "            Average precision score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionchanged:: 0.19\n",
      "          Instead of linearly interpolating between operating points, precisions\n",
      "          are weighted by the change in recall since the last operating point.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n",
      "               oldid=793358396#Average_precision>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)\n",
      "        0.83...\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy.\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, while keeping perfect performance at a score\n",
      "            of 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "            Balanced accuracy score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Compute average precision (AP) from prediction\n",
      "            scores.\n",
      "        precision_score : Compute the precision score.\n",
      "        recall_score : Compute the recall score.\n",
      "        roc_auc_score : Compute Area Under the Receiver Operating Characteristic\n",
      "            Curve (ROC AUC) from prediction scores.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_prob, *, sample_weight=None, pos_label=None)\n",
      "        Compute the Brier score loss.\n",
      "        \n",
      "        The smaller the Brier score loss, the better, hence the naming with \"loss\".\n",
      "        The Brier score measures the mean squared difference between the predicted\n",
      "        probability and the actual outcome. The Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1). It can be decomposed as the sum of refinement loss and\n",
      "        calibration loss.\n",
      "        \n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter `pos_label`, which defaults to\n",
      "        the greater label unless `y_true` is all 0 or all -1, in which case\n",
      "        `pos_label` defaults to 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <brier_score_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array of shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_prob : array of shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label of the positive class. `pos_label` will be inferred in the\n",
      "            following manner:\n",
      "        \n",
      "            * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n",
      "            * else if `y_true` contains string, an error will be raised and\n",
      "              `pos_label` should be explicitly specified;\n",
      "            * otherwise, `pos_label` defaults to the greater label,\n",
      "              i.e. `np.unique(y_true)[-1]`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score loss.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "    \n",
      "    calinski_harabasz_score(X, labels)\n",
      "        Compute the Calinski and Harabasz score.\n",
      "        \n",
      "        It is also known as the Variance Ratio Criterion.\n",
      "        \n",
      "        The score is defined as ratio of the sum of between-cluster dispersion and\n",
      "        of within-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabasz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabasz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "    \n",
      "    check_scoring(estimator, scoring=None, *, allow_none=False)\n",
      "        Determine scorer from user options.\n",
      "        \n",
      "        A TypeError will be thrown if the estimator cannot be scored.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A string (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "            If None, the provided estimator object's `score` method is used.\n",
      "        \n",
      "        allow_none : bool, default=False\n",
      "            If no scoring is specified and the estimator has no score function, we\n",
      "            can either return None or raise an exception.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scoring : callable\n",
      "            A scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "    \n",
      "    class_likelihood_ratios(y_true, y_pred, *, labels=None, sample_weight=None, raise_warning=True)\n",
      "        Compute binary classification positive and negative likelihood ratios.\n",
      "        \n",
      "        The positive likelihood ratio is `LR+ = sensitivity / (1 - specificity)`\n",
      "        where the sensitivity or recall is the ratio `tp / (tp + fn)` and the\n",
      "        specificity is `tn / (tn + fp)`. The negative likelihood ratio is `LR- = (1\n",
      "        - sensitivity) / specificity`. Here `tp` is the number of true positives,\n",
      "        `fp` the number of false positives, `tn` is the number of true negatives and\n",
      "        `fn` the number of false negatives. Both class likelihood ratios can be used\n",
      "        to obtain post-test probabilities given a pre-test probability.\n",
      "        \n",
      "        `LR+` ranges from 1 to infinity. A `LR+` of 1 indicates that the probability\n",
      "        of predicting the positive class is the same for samples belonging to either\n",
      "        class; therefore, the test is useless. The greater `LR+` is, the more a\n",
      "        positive prediction is likely to be a true positive when compared with the\n",
      "        pre-test probability. A value of `LR+` lower than 1 is invalid as it would\n",
      "        indicate that the odds of a sample being a true positive decrease with\n",
      "        respect to the pre-test odds.\n",
      "        \n",
      "        `LR-` ranges from 0 to 1. The closer it is to 0, the lower the probability\n",
      "        of a given sample to be a false negative. A `LR-` of 1 means the test is\n",
      "        useless because the odds of having the condition did not change after the\n",
      "        test. A value of `LR-` greater than 1 invalidates the classifier as it\n",
      "        indicates an increase in the odds of a sample belonging to the positive\n",
      "        class after being classified as negative. This is the case when the\n",
      "        classifier systematically predicts the opposite of the true label.\n",
      "        \n",
      "        A typical application in medicine is to identify the positive/negative class\n",
      "        to the presence/absence of a disease, respectively; the classifier being a\n",
      "        diagnostic test; the pre-test probability of an individual having the\n",
      "        disease can be the prevalence of such disease (proportion of a particular\n",
      "        population found to be affected by a medical condition); and the post-test\n",
      "        probabilities would be the probability that the condition is truly present\n",
      "        given a positive test result.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <class_likelihood_ratios>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            List of labels to index the matrix. This may be used to select the\n",
      "            positive and negative classes with the ordering `labels=[negative_class,\n",
      "            positive_class]`. If `None` is given, those that appear at least once in\n",
      "            `y_true` or `y_pred` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        raise_warning : bool, default=True\n",
      "            Whether or not a case-specific warning message is raised when there is a\n",
      "            zero division. Even if the error is not raised, the function will return\n",
      "            nan in such cases.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        (positive_likelihood_ratio, negative_likelihood_ratio) : tuple\n",
      "            A tuple of two float, the first containing the Positive likelihood ratio\n",
      "            and the second the Negative likelihood ratio.\n",
      "        \n",
      "        Warns\n",
      "        -----\n",
      "        When `false positive == 0`, the positive likelihood ratio is undefined.\n",
      "        When `true negative == 0`, the negative likelihood ratio is undefined.\n",
      "        When `true positive + false negative == 0` both ratios are undefined.\n",
      "        In such cases, `UserWarning` will be raised if raise_warning=True.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Likelihood ratios in diagnostic testing\n",
      "               <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import class_likelihood_ratios\n",
      "        >>> class_likelihood_ratios([0, 1, 0, 1, 0], [1, 1, 0, 0, 0])\n",
      "        (1.5, 0.75)\n",
      "        >>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n",
      "        >>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred)\n",
      "        (1.33..., 0.66...)\n",
      "        >>> y_true = np.array([\"non-zebra\", \"zebra\", \"non-zebra\", \"zebra\", \"non-zebra\"])\n",
      "        >>> y_pred = np.array([\"zebra\", \"zebra\", \"non-zebra\", \"non-zebra\", \"non-zebra\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred)\n",
      "        (1.5, 0.75)\n",
      "        \n",
      "        To avoid ambiguities, use the notation `labels=[negative_class,\n",
      "        positive_class]`\n",
      "        \n",
      "        >>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n",
      "        >>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred, labels=[\"non-cat\", \"cat\"])\n",
      "        (1.5, 0.75)\n",
      "    \n",
      "    classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
      "        Build a text report showing the main classification metrics.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_labels,), default=None\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : list of str of shape (n_labels,), default=None\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int, default=2\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool, default=False\n",
      "            If True, return output as dict.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : str or dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), and sample average (only for multilabel classification).\n",
      "            Micro average (averaging the total true positives, false negatives and\n",
      "            false positives) is only shown for multi-label or multi-class\n",
      "            with a subset of classes, because it corresponds to accuracy\n",
      "            otherwise and would be the same for all metrics.\n",
      "            See also :func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support: Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        confusion_matrix: Compute confusion matrix to evaluate the accuracy of a\n",
      "            classification.\n",
      "        multilabel_confusion_matrix: Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)\n",
      "        Compute Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array of shape (n_samples,)\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array of shape (n_samples,)\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If `None`, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : {'linear', 'quadratic'}, default=None\n",
      "            Weighting type to calculate the score. `None` means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               <10.1177/001316446002000104>`\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Compute completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.999...\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` and\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If ``None`` is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "            conditions or all the population. If None, confusion matrix will not be\n",
      "            normalized.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (n_classes, n_classes)\n",
      "            Confusion matrix whose i-th row and j-th\n",
      "            column entry indicates the number of\n",
      "            samples with true label being i-th class\n",
      "            and predicted label being j-th class.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "            given an estimator, the data, and the label.\n",
      "        ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "            given the true and predicted labels.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, *, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the\n",
      "        best matching between sets is found using the Hungarian algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : 'jaccard' or callable, default='jaccard'\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        consensus_score : float\n",
      "           Consensus score, a non-negative value, sum of similarities\n",
      "           divided by size of larger set.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "    \n",
      "    coverage_error(y_true, y_score, *, sample_weight=None)\n",
      "        Coverage error measure.\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "            The coverage error.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    d2_absolute_error_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        :math:`D^2` regression score function,     fraction of absolute error explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical median of `y_true`\n",
      "        as constant prediction, disregarding the input features,\n",
      "        gets a :math:`D^2` score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.1\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The :math:`D^2` score with an absolute error deviance\n",
      "            or ndarray of scores if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Like :math:`R^2`, :math:`D^2` score may be negative\n",
      "        (it need not actually be the square of a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "         References\n",
      "        ----------\n",
      "        .. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_absolute_error_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        0.764...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.691...\n",
      "        >>> d2_absolute_error_score(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.8125    , 0.57142857])\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        -1.0\n",
      "    \n",
      "    d2_pinball_score(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\n",
      "        :math:`D^2` regression score function, fraction of pinball loss explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical alpha-quantile of\n",
      "        `y_true` as constant prediction, disregarding the input features,\n",
      "        gets a :math:`D^2` score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.1\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        alpha : float, default=0.5\n",
      "            Slope of the pinball deviance. It determines the quantile level alpha\n",
      "            for which the pinball deviance and also D2 are optimal.\n",
      "            The default `alpha=0.5` is equivalent to `d2_absolute_error_score`.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The :math:`D^2` score with a pinball deviance\n",
      "            or ndarray of scores if `multioutput='raw_values'`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Like :math:`R^2`, :math:`D^2` score may be negative\n",
      "        (it need not actually be the square of a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for a single point and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "         References\n",
      "        ----------\n",
      "        .. [1] Eq. (7) of `Koenker, Roger; Machado, José A. F. (1999).\n",
      "               \"Goodness of Fit and Related Inference Processes for Quantile Regression\"\n",
      "               <http://dx.doi.org/10.1080/01621459.1999.10473882>`_\n",
      "        .. [2] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_pinball_score\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 3, 3]\n",
      "        >>> d2_pinball_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> d2_pinball_score(y_true, y_pred, alpha=0.9)\n",
      "        0.772...\n",
      "        >>> d2_pinball_score(y_true, y_pred, alpha=0.1)\n",
      "        -1.045...\n",
      "        >>> d2_pinball_score(y_true, y_true, alpha=0.1)\n",
      "        1.0\n",
      "    \n",
      "    d2_tweedie_score(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        D^2 regression score function, fraction of Tweedie deviance explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical mean of `y_true` as\n",
      "        constant prediction, disregarding the input features, gets a D^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to r2_score.\n",
      "              y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The D^2 score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Like R^2, D^2 score may be negative (it need not actually be the square of\n",
      "        a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_tweedie_score\n",
      "        >>> y_true = [0.5, 1, 2.5, 7]\n",
      "        >>> y_pred = [1, 1, 5, 3.5]\n",
      "        >>> d2_tweedie_score(y_true, y_pred)\n",
      "        0.285...\n",
      "        >>> d2_tweedie_score(y_true, y_pred, power=1)\n",
      "        0.487...\n",
      "        >>> d2_tweedie_score(y_true, y_pred, power=2)\n",
      "        0.630...\n",
      "        >>> d2_tweedie_score(y_true, y_true, power=2)\n",
      "        1.0\n",
      "    \n",
      "    davies_bouldin_score(X, labels)\n",
      "        Compute the Davies-Bouldin score.\n",
      "        \n",
      "        The score is defined as the average similarity measure of each cluster with\n",
      "        its most similar cluster, where similarity is the ratio of within-cluster\n",
      "        distances to between-cluster distances. Thus, clusters which are farther\n",
      "        apart and less dispersed will result in a better score.\n",
      "        \n",
      "        The minimum score is zero, with lower values indicating better clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score: float\n",
      "            The resulting Davies-Bouldin score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
      "           `\"A Cluster Separation Measure\"\n",
      "           <https://ieeexplore.ieee.org/document/4766909>`__.\n",
      "           IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
      "           PAMI-1 (2): 224-227\n",
      "    \n",
      "    dcg_score(y_true, y_score, *, k=None, log_base=2, sample_weight=None, ignore_ties=False)\n",
      "        Compute Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Usually the Normalized Discounted Cumulative Gain (NDCG, computed by\n",
      "        ndcg_score) is preferred.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If None, use all\n",
      "            outputs.\n",
      "        \n",
      "        log_base : float, default=2\n",
      "            Base of the logarithm used for the discount. A low value means a\n",
      "            sharper discount (top results are more important).\n",
      "        \n",
      "        sample_weight : ndarray of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        discounted_cumulative_gain : float\n",
      "            The averaged sample DCG scores.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n",
      "            Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n",
      "            have a score between 0 and 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013).\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import dcg_score\n",
      "        >>> # we have groud-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict scores for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> dcg_score(true_relevance, scores)\n",
      "        9.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute\n",
      "        >>> dcg_score(true_relevance, scores, k=2)\n",
      "        5.63...\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average true\n",
      "        >>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n",
      "        >>> dcg_score(true_relevance, scores, k=1)\n",
      "        7.5\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> dcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        5.0\n",
      "    \n",
      "    det_curve(y_true, y_score, pos_label=None, sample_weight=None)\n",
      "        Compute error rates for different probability thresholds.\n",
      "        \n",
      "        .. note::\n",
      "           This metric is used for evaluation of ranking and error tradeoffs of\n",
      "           a binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <det_curve>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape of (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (n_thresholds,)\n",
      "            False positive rate (FPR) such that element i is the false positive\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false acceptance propability or fall-out.\n",
      "        \n",
      "        fnr : ndarray of shape (n_thresholds,)\n",
      "            False negative rate (FNR) such that element i is the false negative\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false rejection or miss rate.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Decreasing score values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "            some data.\n",
      "        DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "            predicted labels.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        precision_recall_curve : Compute precision-recall curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import det_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, fnr, thresholds = det_curve(y_true, y_scores)\n",
      "        >>> fpr\n",
      "        array([0.5, 0.5, 0. ])\n",
      "        >>> fnr\n",
      "        array([0. , 0.5, 0.5])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Compute the distance matrix between each pair from a vector array X and Y.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation,\n",
      "        because this equation potentially suffers from \"catastrophic cancellation\".\n",
      "        Also, the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "            If `None`, method uses `Y=X`.\n",
      "        \n",
      "        Y_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            Returns the distances between the row vectors of `X`\n",
      "            and the row vectors of `Y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve a better accuracy, `X_norm_squared` and `Y_norm_squared` may be\n",
      "        unused if they are passed as `np.float32`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)\n",
      "        Explained variance regression score function.\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        In the particular case when ``y_true`` is constant, the explained variance\n",
      "        score is not finite: it is either ``NaN`` (perfect predictions) or\n",
      "        ``-Inf`` (imperfect predictions). To prevent such non-finite numbers to\n",
      "        pollute higher-level experiments such as a grid search cross-validation,\n",
      "        by default these cases are replaced with 1.0 (perfect predictions) or 0.0\n",
      "        (imperfect predictions) respectively. If ``force_finite``\n",
      "        is set to ``False``, this score falls back on the original :math:`R^2`\n",
      "        definition.\n",
      "        \n",
      "        .. note::\n",
      "           The Explained Variance score is similar to the\n",
      "           :func:`R^2 score <r2_score>`, with the notable difference that it\n",
      "           does not account for systematic offsets in the prediction. Most often\n",
      "           the :func:`R^2 score <r2_score>` should be preferred.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        force_finite : bool, default=True\n",
      "            Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
      "            data should be replaced with real numbers (``1.0`` if prediction is\n",
      "            perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
      "            for hyperparameters' search procedures (e.g. grid search\n",
      "            cross-validation).\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        r2_score :\n",
      "            Similar metric, but accounting for systematic offsets in\n",
      "            prediction.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.983...\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> explained_variance_score(y_true, y_pred, force_finite=False)\n",
      "        nan\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2 + 1e-8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> explained_variance_score(y_true, y_pred, force_finite=False)\n",
      "        -inf\n",
      "    \n",
      "    f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure.\n",
      "        \n",
      "        The F1 score can be interpreted as a harmonic mean of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is::\n",
      "        \n",
      "            F1 = 2 * (precision * recall) / (precision + recall)\n",
      "        \n",
      "        In the multi-class and multi-label case, this is the average of\n",
      "        the F1 score of each class with weighting depending on the ``average``\n",
      "        parameter.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "            but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fbeta_score : Compute the F-beta score.\n",
      "        precision_recall_fscore_support : Compute the precision, recall, F-score,\n",
      "            and support.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, by default the metric will be set to 0, as will f-score,\n",
      "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> f1_score(y_true, y_pred, zero_division=1)\n",
      "        1.0...\n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.66666667, 1.        , 0.66666667])\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F-beta score.\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter determines the weight of recall in the combined\n",
      "        score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n",
      "        favors recall (``beta -> 0`` considers only precision, ``beta -> +inf``\n",
      "        only recall).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Determines the weight of recall in the combined score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "            but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute the precision, recall, F-score,\n",
      "            and support.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        array([0.71..., 0.        , 0.        ])\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, *, sparse=False)\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n",
      "        points that belongs in the same clusters in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pair of points that belongs in the same clusters in\n",
      "        ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e the number of pair of points that belongs in the\n",
      "        same clusters in ``labels_pred`` and not in ``labels_True``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = (``n_samples``,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = (``n_samples``, )\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool, default=False\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring_parameter>`.\n",
      "        :func:`~sklearn.metrics.get_scorer_names` can be used to retrieve the names\n",
      "        of all available scorers.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str or callable\n",
      "            Scoring method as string. If callable it is returned as is.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When passed a string, this function always returns a copy of the scorer\n",
      "        object. Calling `get_scorer` twice for the same scorer results in two\n",
      "        separate scorer objects.\n",
      "    \n",
      "    get_scorer_names()\n",
      "        Get the names of all available scorers.\n",
      "        \n",
      "        These names can be passed to :func:`~sklearn.metrics.get_scorer` to\n",
      "        retrieve the scorer object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of str\n",
      "            Names of all available scorers.\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "            function will return the percentage of imperfectly predicted subsets.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, *, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized).\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array of shape (n_samples,)\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Average hinge loss.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_.\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292.\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero\n",
      "               <https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)\n",
      "        LinearSVC(random_state=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)\n",
      "        LinearSVC()\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels=labels)\n",
      "        0.56...\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred, *, beta=1.0)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness. V-Measure is identical to\n",
      "        :func:`normalized_mutual_info_score` with the arithmetic averaging\n",
      "        method.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Gluster labels to evaluate.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "            Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.\n",
      "        \n",
      "        completeness : float\n",
      "            Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        v_measure : float\n",
      "            Harmonic mean of the first two.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          1.000000\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          1.000000\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Jaccard similarity coefficient score.\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted',             'binary'} or None, default='binary'\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when there\n",
      "            there are no negative values in predictions and labels. If set to\n",
      "            \"warn\", this acts like 0, but a warning is also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of shape (n_unique_labels,), dtype=np.float64\n",
      "            The Jaccard score. When `average` is not `None`, a single scalar is\n",
      "            returned.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Function for calculating the accuracy score.\n",
      "        f1_score : Function for calculating the F1 score.\n",
      "        multilabel_confusion_matrix : Function for computing a confusion matrix                                  for each class or sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])\n",
      "        0.6666...\n",
      "        \n",
      "        In the 2D comparison case (e.g. image similarity):\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average=\"micro\")\n",
      "        0.6\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833...\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666...\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([1. , 0. , 0.33...])\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None)\n",
      "        Compute ranking-based average precision.\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Ranking-based average precision score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)\n",
      "        0.416...\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, *, sample_weight=None)\n",
      "        Compute Ranking loss measure.\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Average number of label pairs that are incorrectly ordered given\n",
      "            y_score weighted by the size of the label set and the number of labels not\n",
      "            in the label set.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    log_loss(y_true, y_pred, *, eps='auto', normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of a logistic model that returns ``y_pred`` probabilities\n",
      "        for its training data ``y_true``.\n",
      "        The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label :math:`y \\in \\{0,1\\}` and\n",
      "        a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\n",
      "        loss is:\n",
      "        \n",
      "        .. math::\n",
      "            L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        eps : float or \"auto\", default=\"auto\"\n",
      "            Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "            clipped to `max(eps, min(1 - eps, p))`. The default will depend on the\n",
      "            data type of `y_pred` and is set to `np.finfo(y_pred.dtype).eps`.\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "        \n",
      "            .. versionchanged:: 1.2\n",
      "               The default value changed from `1e-15` to `\"auto\"` that is\n",
      "               equivalent to `np.finfo(y_pred.dtype).eps`.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "    \n",
      "    make_scorer(score_func, *, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        This factory function wraps scoring functions for use in\n",
      "        :class:`~sklearn.model_selection.GridSearchCV` and\n",
      "        :func:`~sklearn.model_selection.cross_val_score`.\n",
      "        It takes a score function, such as :func:`~sklearn.metrics.accuracy_score`,\n",
      "        :func:`~sklearn.metrics.mean_squared_error`,\n",
      "        :func:`~sklearn.metrics.adjusted_rand_score` or\n",
      "        :func:`~sklearn.metrics.average_precision_score`\n",
      "        and returns a callable that scores an estimator's output.\n",
      "        The signature of the call is `(estimator, X, y)` where `estimator`\n",
      "        is the model to be evaluated, `X` is the data and `y` is the\n",
      "        ground truth labeling (or `None` in the case of unsupervised models).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable\n",
      "            Score function (or loss function) with signature\n",
      "            `score_func(y, y_pred, **kwargs)`.\n",
      "        \n",
      "        greater_is_better : bool, default=True\n",
      "            Whether `score_func` is a score function (default), meaning high is\n",
      "            good, or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the `score_func`.\n",
      "        \n",
      "        needs_proba : bool, default=False\n",
      "            Whether `score_func` requires `predict_proba` to get probability\n",
      "            estimates out of a classifier.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class, shape\n",
      "            `(n_samples,)`).\n",
      "        \n",
      "        needs_threshold : bool, default=False\n",
      "            Whether `score_func` takes a continuous decision certainty.\n",
      "            This only works for binary classification using estimators that\n",
      "            have either a `decision_function` or `predict_proba` method.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class or the decision\n",
      "            function, shape `(n_samples,)`).\n",
      "        \n",
      "            For example `average_precision` or the area under the roc curve\n",
      "            can not be computed using discrete predictions alone.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to `score_func`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If `needs_proba=False` and `needs_threshold=False`, the score\n",
      "        function is supposed to accept the output of :term:`predict`. If\n",
      "        `needs_proba=True`, the score function is supposed to accept the\n",
      "        output of :term:`predict_proba` (For binary `y_true`, the score function is\n",
      "        supposed to accept probability of the positive class). If\n",
      "        `needs_threshold=True`, the score function is supposed to accept the\n",
      "        output of :term:`decision_function` or :term:`predict_proba` when\n",
      "        :term:`decision_function` is not present.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC).\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview.\n",
      "           <10.1093/bioinformatics/16.5.412>`\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_.\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)\n",
      "        -0.33...\n",
      "    \n",
      "    max_error(y_true, y_pred)\n",
      "        The max_error metric calculates the maximum residual error.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <max_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        max_error : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import max_error\n",
      "        >>> y_true = [3, 2, 7, 1]\n",
      "        >>> y_pred = [4, 2, 7, 1]\n",
      "        >>> max_error(y_true, y_pred)\n",
      "        1\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85...\n",
      "    \n",
      "    mean_absolute_percentage_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute percentage error (MAPE) regression loss.\n",
      "        \n",
      "        Note here that the output is not a percentage in the range [0, 100]\n",
      "        and a value of 100 does not mean 100% but 1e2. Furthermore, the output\n",
      "        can be arbitrarily high when `y_true` is small (which is specific to the\n",
      "        metric) or when `abs(y_true - y_pred)` is large (which is common for most\n",
      "        regression metrics). Read more in the\n",
      "        :ref:`User Guide <mean_absolute_percentage_error>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "            If input is list then the shape must be (n_outputs,).\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute percentage error\n",
      "            is returned for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAPE output is non-negative floating point. The best value is 0.0.\n",
      "            But note that bad predictions can lead to arbitrarily large\n",
      "            MAPE values, especially if some `y_true` values are very close to zero.\n",
      "            Note that we return a large value instead of `inf` when `y_true` is zero.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_percentage_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.3273...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.5515...\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.6198...\n",
      "        >>> # the value when some element of the y_true is zero is arbitrarily high because\n",
      "        >>> # of the division by epsilon\n",
      "        >>> y_true = [1., 0., 2.4, 7.]\n",
      "        >>> y_pred = [1.2, 0.1, 2.4, 8.]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        112589990684262.48\n",
      "    \n",
      "    mean_gamma_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Gamma deviance regression loss.\n",
      "        \n",
      "        Gamma deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=2`. It is invariant to scaling of\n",
      "        the target variable, and measures relative errors.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true > 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_gamma_deviance\n",
      "        >>> y_true = [2, 0.5, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_gamma_deviance(y_true, y_pred)\n",
      "        1.0568...\n",
      "    \n",
      "    mean_pinball_loss(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\n",
      "        Pinball loss for quantile regression.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pinball_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        alpha : float, slope of the pinball loss, default=0.5,\n",
      "            This loss is equivalent to :ref:`mean_absolute_error` when `alpha=0.5`,\n",
      "            `alpha=0.95` is minimized by estimators of the 95th percentile.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            The pinball loss output is a non-negative floating point. The best\n",
      "            value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_pinball_loss\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\n",
      "        0.03...\n",
      "        >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\n",
      "        0.3...\n",
      "        >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\n",
      "        0.3...\n",
      "        >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\n",
      "        0.03...\n",
      "        >>> mean_pinball_loss(y_true, y_true, alpha=0.1)\n",
      "        0.0\n",
      "        >>> mean_pinball_loss(y_true, y_true, alpha=0.9)\n",
      "        0.0\n",
      "    \n",
      "    mean_poisson_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Poisson deviance regression loss.\n",
      "        \n",
      "        Poisson deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=1`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true >= 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_poisson_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_poisson_deviance(y_true, y_pred)\n",
      "        1.4260...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared=True)\n",
      "        Mean squared error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        squared : bool, default=True\n",
      "            If True returns MSE value, if False returns RMSE value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "        0.612...\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "        0.822...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.41666667, 1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.825...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared=True)\n",
      "        Mean squared logarithmic error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        squared : bool, default=True\n",
      "            If True returns MSLE (mean squared log error) value.\n",
      "            If False returns RMSLE (root mean squared log error) value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.039...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, squared=False)\n",
      "        0.199...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.00462428, 0.08377444])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.060...\n",
      "    \n",
      "    mean_tweedie_deviance(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        Mean Tweedie deviance regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to\n",
      "              mean_squared_error. y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_tweedie_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_tweedie_deviance(y_true, y_pred, power=1)\n",
      "        1.4260...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred, *, multioutput='uniform_average', sample_weight=None)\n",
      "        Median absolute error regression loss.\n",
      "        \n",
      "        Median absolute error output is non-negative floating point. The best value\n",
      "        is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values. Array-like value defines\n",
      "            weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data).\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : ndarray of shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix : Compute confusion matrix to evaluate the accuracy of a\n",
      "            classifier.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `multilabel_confusion_matrix` calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while\n",
      "        :func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix\n",
      "        for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, *, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels\n",
      "        of the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching :math:`U` (i.e\n",
      "        ``label_true``) with :math:`V` (i.e. ``label_pred``) will return the\n",
      "        same score value. This can be useful to measure the agreement of two\n",
      "        independent label assignments strategies on the same dataset when the\n",
      "        real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets, called :math:`U` in\n",
      "            the above formula.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets, called :math:`V` in\n",
      "            the above formula.\n",
      "        \n",
      "        contingency : {ndarray, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None\n",
      "            A contingency matrix given by the :func:`contingency_matrix` function.\n",
      "            If value is ``None``, it will be computed, otherwise the given value is\n",
      "            used, with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value, measured in nats using the\n",
      "           natural logarithm.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted against chance Mutual Information.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "    \n",
      "    nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)\n",
      "        Calculate the euclidean distances in the presence of missing values.\n",
      "        \n",
      "        Compute the euclidean distance between each pair of samples in X and Y,\n",
      "        where Y=X is assumed if Y=None. When calculating the distance between a\n",
      "        pair of samples, this formulation ignores feature coordinates with a\n",
      "        missing value in either sample and scales up the weight of the remaining\n",
      "        coordinates:\n",
      "        \n",
      "            dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n",
      "            where,\n",
      "            weight = Total # of coordinates / # of present coordinates\n",
      "        \n",
      "        For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\n",
      "        is:\n",
      "        \n",
      "            .. math::\n",
      "                \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n",
      "        \n",
      "        If all the coordinates are missing or if there are no common present\n",
      "        coordinates then NaN is returned for that pair.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        .. versionadded:: 0.22\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, n_features), default=None\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "            If `None`, method uses `Y=X`.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        missing_values : np.nan or int, default=np.nan\n",
      "            Representation of missing value.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Make and use a deep copy of X and Y (if Y exists).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            Returns the distances between the row vectors of `X`\n",
      "            and the row vectors of `Y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n",
      "          IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n",
      "          10, pp. 617 - 621, Oct. 1979.\n",
      "          http://ieeexplore.ieee.org/abstract/document/4310090/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import nan_euclidean_distances\n",
      "        >>> nan = float(\"NaN\")\n",
      "        >>> X = [[0, 1], [1, nan]]\n",
      "        >>> nan_euclidean_distances(X, X) # distance between rows of X\n",
      "        array([[0.        , 1.41421356],\n",
      "               [1.41421356, 0.        ]])\n",
      "        \n",
      "        >>> # get distance to origin\n",
      "        >>> nan_euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False)\n",
      "        Compute Normalized Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount. Then divide by the best possible\n",
      "        score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n",
      "        0 and 1.\n",
      "        \n",
      "        This ranking metric returns a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked. Negative values in `y_true` may result in an output\n",
      "            that is not between 0 and 1.\n",
      "        \n",
      "            .. versionchanged:: 1.2\n",
      "                These negative values are deprecated, and will raise an error in v1.4.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If `None`, use all\n",
      "            outputs.\n",
      "        \n",
      "        sample_weight : ndarray of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        normalized_discounted_cumulative_gain : float in [0., 1.]\n",
      "            The averaged NDCG scores for all samples.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        dcg_score : Discounted Cumulative Gain (not normalized).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013)\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import ndcg_score\n",
      "        >>> # we have groud-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict some scores (relevance) for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.69...\n",
      "        >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute.\n",
      "        >>> ndcg_score(true_relevance, scores, k=4)\n",
      "        0.35...\n",
      "        >>> # the normalization takes k into account so a perfect answer\n",
      "        >>> # would still get 1.0\n",
      "        >>> ndcg_score(true_relevance, true_relevance, k=4)\n",
      "        1.0...\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average (normalized)\n",
      "        >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n",
      "        >>> ndcg_score(true_relevance, scores, k=1)\n",
      "        0.75...\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> ndcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        0.5...\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is a normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by some generalized mean of ``H(labels_true)``\n",
      "        and ``H(labels_pred))``, defined by the `average_method`.\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mutual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : str, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'geometric' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           Score between 0.0 and 1.0 in normalized nats (based on the natural\n",
      "           logarithm). 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information (adjusted\n",
      "            against chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    pair_confusion_matrix(labels_true, labels_pred)\n",
      "        Pair confusion matrix arising from two clusterings [1]_.\n",
      "        \n",
      "        The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\n",
      "        between two clusterings by considering all pairs of samples and counting\n",
      "        pairs that are assigned into the same or into different clusters under\n",
      "        the true and predicted clusterings.\n",
      "        \n",
      "        Considering a pair of samples that is clustered together a positive pair,\n",
      "        then as in binary classification the count of true negatives is\n",
      "        :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n",
      "        :math:`C_{11}` and false positives is :math:`C_{01}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pair_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (2, 2), dtype=np.int64\n",
      "            The contingency matrix.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rand_score: Rand Score.\n",
      "        adjusted_rand_score: Adjusted Rand Score.\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"\n",
      "               Journal of Classification 2, 193–218 (1985).\n",
      "               <10.1007/BF01908075>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have all non-zero entries on the\n",
      "        diagonal regardless of actual label values:\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import pair_confusion_matrix\n",
      "          >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          array([[8, 0],\n",
      "                 [0, 4]]...\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may be not always pure, hence penalized, and\n",
      "        have some off-diagonal non-zero entries:\n",
      "        \n",
      "          >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          array([[8, 2],\n",
      "                 [0, 2]]...\n",
      "        \n",
      "        Note that the matrix is not symmetric.\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix\n",
      "          inputs.\n",
      "          ['nan_euclidean'] but it does not yet support sparse matrices.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see :func:`sklearn.metrics.pairwise.distance_metrics`\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n",
      "            for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances_chunked : Performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        paired_distances : Computes the distances between corresponding elements\n",
      "            of two arrays.\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default=\"euclidean\"\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances : Distances between every pair of samples of X and Y.\n",
      "        pairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also\n",
      "            returns the distances.\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : ndarray\n",
      "            The array of minimum distances. `distances[i]` is the distance between\n",
      "            the i-th row in X and the argmin[i]-th row in Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances : Distances between every pair of samples of X and Y.\n",
      "        pairwise_distances_argmin : Same as `pairwise_distances_argmin_min` but only\n",
      "            returns the argmins.\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction.\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be\n",
      "        stored at once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is\n",
      "        run on each chunk and its return values are concatenated into lists,\n",
      "        arrays or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape the array should be (n_samples_X, n_samples_X) if\n",
      "            metric='precomputed' and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, default=None\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return one of: None; an array, a list, or a sparse matrix\n",
      "            of length ``D_chunk.shape[0]``; or a tuple of such objects.\n",
      "            Returning None is useful for in-place operations, rather than\n",
      "            reductions.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter,\n",
      "            or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on\n",
      "            each pair of instances (rows) and the resulting value recorded.\n",
      "            The callable should take two arrays from X as input and return a\n",
      "            value indicating the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by\n",
      "            breaking down the pairwise matrix into n_jobs even slices and\n",
      "            computing them in parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : {ndarray, sparse matrix}\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "            'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_features)\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            A second feature array only if X has shape (n_samples_X, n_features).\n",
      "        \n",
      "        metric : str or callable, default=\"linear\"\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two rows from X as input and return the corresponding\n",
      "            kernel value as a single number. This means that callables from\n",
      "            :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n",
      "            matrices, not single samples. Use the string identifying the kernel\n",
      "            instead.\n",
      "        \n",
      "        filter_params : bool, default=False\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    precision_recall_curve(y_true, probas_pred, *, pos_label=None, sample_weight=None)\n",
      "        Compute precision-recall pairs for different probability thresholds.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold. This ensures that the graph starts on the\n",
      "        y axis.\n",
      "        \n",
      "        The first precision and recall values are precision=class balance and recall=1.0\n",
      "        which corresponds to a classifier that always predicts the positive class.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        probas_pred : ndarray of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, or non-thresholded measure of decisions (as returned by\n",
      "            `decision_function` on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : ndarray of shape (n_thresholds + 1,)\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : ndarray of shape (n_thresholds + 1,)\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall where `n_thresholds = len(np.unique(probas_pred))`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n",
      "            a binary classifier.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n",
      "            using predictions from a binary classifier.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision\n",
      "        array([0.5       , 0.66666667, 0.5       , 1.        , 1.        ])\n",
      "        >>> recall\n",
      "        array([1. , 1. , 0.5, 0.5, 0. ])\n",
      "        >>> thresholds\n",
      "        array([0.1 , 0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
      "        Compute precision, recall, F-measure and support for each class.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label a negative sample as\n",
      "        positive.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        If ``pos_label is None`` and in binary classification, this function\n",
      "        returns the average precision, recall and F-measure if ``average``\n",
      "        is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'binary', 'micro', 'macro', 'samples', 'weighted'},             default=None\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division:\n",
      "               - recall: when there are no positive labels\n",
      "               - precision: when there are no positive predictions\n",
      "               - f-score: both\n",
      "        \n",
      "            If set to \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Precision score.\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Recall score.\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score.\n",
      "        \n",
      "        support : None (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, by default the metric will be set to 0, as will f-score,\n",
      "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        (array([0.        , 0.        , 0.66...]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "    \n",
      "    precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the precision.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n",
      "            number of true positives and ``fn`` the number of false negatives.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "            an estimator and some data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "            binary class predictions.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.66..., 0.        , 0.        ])\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.33..., 0.        , 0.        ])\n",
      "        >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.33..., 1.        , 1.        ])\n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 1. , 1. ])\n",
      "    \n",
      "    r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)\n",
      "        :math:`R^2` (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). In the general case when the true y is\n",
      "        non-constant, a constant model that always predicts the average y\n",
      "        disregarding the input features would get a :math:`R^2` score of 0.0.\n",
      "        \n",
      "        In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
      "        is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
      "        (imperfect predictions). To prevent such non-finite numbers to pollute\n",
      "        higher-level experiments such as a grid search cross-validation, by default\n",
      "        these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
      "        predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
      "        prevent this fix from happening.\n",
      "        \n",
      "        Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
      "        is identical to the\n",
      "        :func:`Explained Variance score <explained_variance_score>`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        force_finite : bool, default=True\n",
      "            Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
      "            data should be replaced with real numbers (``1.0`` if prediction is\n",
      "            perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
      "            for hyperparameters' search procedures (e.g. grid search\n",
      "            cross-validation).\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
      "        actually be the square of a quantity R).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred,\n",
      "        ...          multioutput='variance_weighted')\n",
      "        0.938...\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> r2_score(y_true, y_pred, force_finite=False)\n",
      "        nan\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2 + 1e-8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> r2_score(y_true, y_pred, force_finite=False)\n",
      "        -inf\n",
      "    \n",
      "    rand_score(labels_true, labels_pred)\n",
      "        Rand index.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings [1]_ [2]_.\n",
      "        \n",
      "        The raw RI score [3]_ is:\n",
      "        \n",
      "            RI = (number of agreeing pairs) / (number of pairs)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        RI : float\n",
      "           Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\n",
      "           perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Score.\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"\n",
      "           Journal of Classification 2, 193–218 (1985).\n",
      "           <10.1007/BF01908075>`.\n",
      "        \n",
      "        .. [2] `Wikipedia: Simple Matching Coefficient\n",
      "            <https://en.wikipedia.org/wiki/Simple_matching_coefficient>`_\n",
      "        \n",
      "        .. [3] `Wikipedia: Rand Index <https://en.wikipedia.org/wiki/Rand_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import rand_score\n",
      "          >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized:\n",
      "        \n",
      "          >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.83...\n",
      "    \n",
      "    recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the recall.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall. Weighted recall\n",
      "                is equal to accuracy.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float of shape              (n_unique_labels,)\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the\n",
      "            number of true positives and ``fp`` the number of false positives.\n",
      "        balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced\n",
      "            datasets.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "            an estimator and some data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "            binary class predictions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be modified with\n",
      "        ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0. , 0. ])\n",
      "        >>> recall_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.5, 1. , 1. ])\n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1. , 1. , 0.5])\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
      "        Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\n",
      "        \n",
      "        Note: this implementation can be used with binary, multiclass and\n",
      "        multilabel classification, but some restrictions apply (see Parameters).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True labels or binary label indicators. The binary and multiclass cases\n",
      "            expect labels with shape (n_samples,) while the multilabel case expects\n",
      "            binary label indicators with shape (n_samples, n_classes).\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores.\n",
      "        \n",
      "            * In the binary case, it corresponds to an array of shape\n",
      "              `(n_samples,)`. Both probability estimates and non-thresholded\n",
      "              decision values can be provided. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label**,\n",
      "              i.e. `estimator.classes_[1]` and thus\n",
      "              `estimator.predict_proba(X, y)[:, 1]`. The decision values\n",
      "              corresponds to the output of `estimator.decision_function(X, y)`.\n",
      "              See more information in the :ref:`User guide <roc_auc_binary>`;\n",
      "            * In the multiclass case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)` of probability estimates provided by the\n",
      "              `predict_proba` method. The probability estimates **must**\n",
      "              sum to 1 across the possible classes. In addition, the order of the\n",
      "              class scores must correspond to the order of ``labels``,\n",
      "              if provided, or else to the numerical or lexicographical order of\n",
      "              the labels in ``y_true``. See more information in the\n",
      "              :ref:`User guide <roc_auc_multiclass>`;\n",
      "            * In the multilabel case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)`. Probability estimates are provided by the\n",
      "              `predict_proba` method and the non-thresholded decision values by\n",
      "              the `decision_function` method. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label for each\n",
      "              output** of the classifier. See more information in the\n",
      "              :ref:`User guide <roc_auc_multilabel>`.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned.\n",
      "            Otherwise, this determines the type of averaging performed on the data.\n",
      "            Note: multiclass ROC AUC currently only handles the 'macro' and\n",
      "            'weighted' averages. For multiclass targets, `average=None` is only\n",
      "            implemented for `multi_class='ovr'` and `average='micro'` is only\n",
      "            implemented for `multi_class='ovr'`.\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        max_fpr : float > 0 and <= 1, default=None\n",
      "            If not ``None``, the standardized partial AUC [2]_ over the range\n",
      "            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n",
      "            should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n",
      "            computation currently is not supported for multiclass.\n",
      "        \n",
      "        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'\n",
      "            Only used for multiclass targets. Determines the type of configuration\n",
      "            to use. The default value raises an error, so either\n",
      "            ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n",
      "        \n",
      "            ``'ovr'``:\n",
      "                Stands for One-vs-rest. Computes the AUC of each class\n",
      "                against the rest [3]_ [4]_. This\n",
      "                treats the multiclass case in the same way as the multilabel case.\n",
      "                Sensitive to class imbalance even when ``average == 'macro'``,\n",
      "                because class imbalance affects the composition of each of the\n",
      "                'rest' groupings.\n",
      "            ``'ovo'``:\n",
      "                Stands for One-vs-one. Computes the average AUC of all\n",
      "                possible pairwise combinations of classes [5]_.\n",
      "                Insensitive to class imbalance when\n",
      "                ``average == 'macro'``.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Only used for multiclass targets. List of labels that index the\n",
      "            classes in ``y_score``. If ``None``, the numerical or lexicographical\n",
      "            order of the labels in ``y_true`` is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "            Area Under the Curve score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "        \n",
      "        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n",
      "               probability estimation trees (Section 6.2), CeDER Working Paper\n",
      "               #IS-00-04, Stern School of Business, New York University.\n",
      "        \n",
      "        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n",
      "                Recognition Letters, 27(8), 861-874.\n",
      "                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n",
      "        \n",
      "        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n",
      "                Under the ROC Curve for Multiple Class Classification Problems.\n",
      "                Machine Learning, 45(2), 171-186.\n",
      "                <http://link.springer.com/article/10.1023/A:1010920819831>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Binary case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_breast_cancer\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
      "        0.99...\n",
      "        >>> roc_auc_score(y, clf.decision_function(X))\n",
      "        0.99...\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
      "        0.99...\n",
      "        \n",
      "        Multilabel case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.datasets import make_multilabel_classification\n",
      "        >>> from sklearn.multioutput import MultiOutputClassifier\n",
      "        >>> X, y = make_multilabel_classification(random_state=0)\n",
      "        >>> clf = MultiOutputClassifier(clf).fit(X, y)\n",
      "        >>> # get a list of n_output containing probability arrays of shape\n",
      "        >>> # (n_samples, n_classes)\n",
      "        >>> y_pred = clf.predict_proba(X)\n",
      "        >>> # extract the positive columns for each output\n",
      "        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])\n",
      "        >>> roc_auc_score(y, y_pred, average=None)\n",
      "        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n",
      "        >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "        >>> clf = RidgeClassifierCV().fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.decision_function(X), average=None)\n",
      "        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\n",
      "    \n",
      "    roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC).\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=True\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (>2,)\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        tpr : ndarray of shape (>2,)\n",
      "            Increasing true positive rates such that element `i` is the true\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        thresholds : ndarray of shape = (n_thresholds,)\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
      "            and is arbitrarily set to `max(y_score) + 1`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([0. , 0. , 0.5, 0.5, 1. ])\n",
      "        >>> tpr\n",
      "        array([0. , 0.5, 0.5, 1. , 1. ])\n",
      "        >>> thresholds\n",
      "        array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n",
      "    \n",
      "    silhouette_samples(X, labels, *, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 ``<= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Label values for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.\n",
      "            If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n",
      "            Precomputed distance matrices must have 0 along the diagonal.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array-like of shape (n_samples,)\n",
      "            Silhouette Coefficients for each sample.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    silhouette_score(X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is ``2 <= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`metrics.pairwise.pairwise_distances\n",
      "            <sklearn.metrics.pairwise.pairwise_distances>`. If ``X`` is\n",
      "            the distance array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int, default=None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for selecting a subset of samples.\n",
      "            Used when ``sample_size is not None``.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    top_k_accuracy_score(y_true, y_score, *, k=2, normalize=True, sample_weight=None, labels=None)\n",
      "        Top-k Accuracy classification score.\n",
      "        \n",
      "        This metric computes the number of times where the correct label is among\n",
      "        the top `k` labels predicted (ranked by predicted scores). Note that the\n",
      "        multilabel case isn't covered here.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <top_k_accuracy_score>`\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True labels.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores. These can be either probability estimates or\n",
      "            non-thresholded decision values (as returned by\n",
      "            :term:`decision_function` on some classifiers).\n",
      "            The binary case expects scores with shape (n_samples,) while the\n",
      "            multiclass case expects scores with shape (n_samples, n_classes).\n",
      "            In the multiclass case, the order of the class scores must\n",
      "            correspond to the order of ``labels``, if provided, or else to\n",
      "            the numerical or lexicographical order of the labels in ``y_true``.\n",
      "            If ``y_true`` does not contain all the labels, ``labels`` must be\n",
      "            provided.\n",
      "        \n",
      "        k : int, default=2\n",
      "            Number of most likely outcomes considered to find the correct label.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If `True`, return the fraction of correctly classified samples.\n",
      "            Otherwise, return the number of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Multiclass only. List of labels that index the classes in ``y_score``.\n",
      "            If ``None``, the numerical or lexicographical order of the labels in\n",
      "            ``y_true`` is used. If ``y_true`` does not contain all the labels,\n",
      "            ``labels`` must be provided.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The top-k accuracy score. The best performance is 1 with\n",
      "            `normalize == True` and the number of samples with\n",
      "            `normalize == False`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In cases where two or more labels are assigned equal predicted scores,\n",
      "        the labels with the highest indices will be chosen first. This might\n",
      "        impact the result if the correct label falls after the threshold because\n",
      "        of that.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import top_k_accuracy_score\n",
      "        >>> y_true = np.array([0, 1, 2, 2])\n",
      "        >>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
      "        ...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n",
      "        ...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n",
      "        ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2)\n",
      "        0.75\n",
      "        >>> # Not normalizing gives the number of \"correctly\" classified samples\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
      "        3\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred, *, beta=1.0)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score` with\n",
      "        the ``'arithmetic'`` option for averaging.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = (1 + beta) * homogeneity * completeness\n",
      "                 / (beta * homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.66...\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harm completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          0.66...\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0...\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "            two sets of samples.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    SCORERS = {'explained_variance': make_scorer(explained_var...jaccard_s...\n",
      "    __all__ = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_r...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\tarun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages\\sklearn\\metrics\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "help(sklearn.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
